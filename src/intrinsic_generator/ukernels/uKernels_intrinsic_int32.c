#include "uKernels_intrinsic_int32.h"
#define Crref(i,j)  Cr[j*ldC+i]
#define vinit_int32(vreg, value)   vreg = vmovq_n_s32(value)
#define vloadC_int32(vreg, mem)    vreg=vld1q_s32(mem)
#define vstoreC_int32(mem, vreg)   vst1q_s32(mem, vreg)
#define vload_int32(vreg, mem) vreg=vld1q_s32(mem)
#define vupdate_lane_int32(vreg1, vreg2, vreg3, lane) vreg1=vmlaq_laneq_s32(vreg1, vreg2, vreg3, lane)

void ukernel_intrinsic_4x4_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=4;
  const int NR=4;
  int32_t Ctmp[4*4];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  B0;
  int32x4_t  C00,  C01,  C02,  C03;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(B0, &Br[bB + 0]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    bA+=4;
    bB+=4;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_4x8_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=4;
  const int NR=8;
  int32_t Ctmp[4*8];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  B0,  B1;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    bA+=4;
    bB+=8;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_4x12_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=4;
  const int NR=12;
  int32_t Ctmp[4*12];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  B0,  B1,  B2;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    bA+=4;
    bB+=12;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_4x16_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=4;
  const int NR=16;
  int32_t Ctmp[4*16];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  B0,  B1,  B2,  B3;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C012,  C013,  C014,  C015;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C012, 0);
    vinit_int32(C013, 0);
    vinit_int32(C014, 0);
    vinit_int32(C015, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C012, &Crref(0, 12));
    vloadC_int32(C013, &Crref(0, 13));
    vloadC_int32(C014, &Crref(0, 14));
    vloadC_int32(C015, &Crref(0, 15));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vload_int32(B3, &Br[bB + 12]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C012, A0, B3, 0); 
    vupdate_lane_int32(C013, A0, B3, 1); 
    vupdate_lane_int32(C014, A0, B3, 2); 
    vupdate_lane_int32(C015, A0, B3, 3); 
    bA+=4;
    bB+=16;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(0,12), C012); 
  vstoreC_int32(&Crref(0,13), C013); 
  vstoreC_int32(&Crref(0,14), C014); 
  vstoreC_int32(&Crref(0,15), C015); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_4x20_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=4;
  const int NR=20;
  int32_t Ctmp[4*20];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  B0,  B1,  B2,  B3,  B4;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C012,  C013,  C014,  C015,  C016,  C017,  C018,  C019;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C012, 0);
    vinit_int32(C013, 0);
    vinit_int32(C014, 0);
    vinit_int32(C015, 0);
    vinit_int32(C016, 0);
    vinit_int32(C017, 0);
    vinit_int32(C018, 0);
    vinit_int32(C019, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C012, &Crref(0, 12));
    vloadC_int32(C013, &Crref(0, 13));
    vloadC_int32(C014, &Crref(0, 14));
    vloadC_int32(C015, &Crref(0, 15));
    vloadC_int32(C016, &Crref(0, 16));
    vloadC_int32(C017, &Crref(0, 17));
    vloadC_int32(C018, &Crref(0, 18));
    vloadC_int32(C019, &Crref(0, 19));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vload_int32(B3, &Br[bB + 12]);
    vload_int32(B4, &Br[bB + 16]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C012, A0, B3, 0); 
    vupdate_lane_int32(C013, A0, B3, 1); 
    vupdate_lane_int32(C014, A0, B3, 2); 
    vupdate_lane_int32(C015, A0, B3, 3); 
    vupdate_lane_int32(C016, A0, B4, 0); 
    vupdate_lane_int32(C017, A0, B4, 1); 
    vupdate_lane_int32(C018, A0, B4, 2); 
    vupdate_lane_int32(C019, A0, B4, 3); 
    bA+=4;
    bB+=20;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(0,12), C012); 
  vstoreC_int32(&Crref(0,13), C013); 
  vstoreC_int32(&Crref(0,14), C014); 
  vstoreC_int32(&Crref(0,15), C015); 
  vstoreC_int32(&Crref(0,16), C016); 
  vstoreC_int32(&Crref(0,17), C017); 
  vstoreC_int32(&Crref(0,18), C018); 
  vstoreC_int32(&Crref(0,19), C019); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_8x4_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=8;
  const int NR=4;
  int32_t Ctmp[8*4];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  B0;
  int32x4_t  C00,  C01,  C02,  C03,  C10,  C11,  C12,  C13;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(B0, &Br[bB + 0]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    bA+=8;
    bB+=4;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_8x8_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=8;
  const int NR=8;
  int32_t Ctmp[8*8];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  B0,  B1;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    bA+=8;
    bB+=8;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_8x12_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=8;
  const int NR=12;
  int32_t Ctmp[8*12];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  B0,  B1,  B2;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C18,  C19,  C110,  C111;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C18, 0);
    vinit_int32(C19, 0);
    vinit_int32(C110, 0);
    vinit_int32(C111, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C18, &Crref(4, 8));
    vloadC_int32(C19, &Crref(4, 9));
    vloadC_int32(C110, &Crref(4, 10));
    vloadC_int32(C111, &Crref(4, 11));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C18, A1, B2, 0); 
    vupdate_lane_int32(C19, A1, B2, 1); 
    vupdate_lane_int32(C110, A1, B2, 2); 
    vupdate_lane_int32(C111, A1, B2, 3); 
    bA+=8;
    bB+=12;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(4,8), C18); 
  vstoreC_int32(&Crref(4,9), C19); 
  vstoreC_int32(&Crref(4,10), C110); 
  vstoreC_int32(&Crref(4,11), C111); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_8x16_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=8;
  const int NR=16;
  int32_t Ctmp[8*16];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  B0,  B1,  B2,  B3;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C012,  C013,  C014,  C015,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C18,  C19,  C110,  C111,  C112,  C113,  C114,  C115;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C012, 0);
    vinit_int32(C013, 0);
    vinit_int32(C014, 0);
    vinit_int32(C015, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C18, 0);
    vinit_int32(C19, 0);
    vinit_int32(C110, 0);
    vinit_int32(C111, 0);
    vinit_int32(C112, 0);
    vinit_int32(C113, 0);
    vinit_int32(C114, 0);
    vinit_int32(C115, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C012, &Crref(0, 12));
    vloadC_int32(C013, &Crref(0, 13));
    vloadC_int32(C014, &Crref(0, 14));
    vloadC_int32(C015, &Crref(0, 15));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C18, &Crref(4, 8));
    vloadC_int32(C19, &Crref(4, 9));
    vloadC_int32(C110, &Crref(4, 10));
    vloadC_int32(C111, &Crref(4, 11));
    vloadC_int32(C112, &Crref(4, 12));
    vloadC_int32(C113, &Crref(4, 13));
    vloadC_int32(C114, &Crref(4, 14));
    vloadC_int32(C115, &Crref(4, 15));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vload_int32(B3, &Br[bB + 12]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C012, A0, B3, 0); 
    vupdate_lane_int32(C013, A0, B3, 1); 
    vupdate_lane_int32(C014, A0, B3, 2); 
    vupdate_lane_int32(C015, A0, B3, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C18, A1, B2, 0); 
    vupdate_lane_int32(C19, A1, B2, 1); 
    vupdate_lane_int32(C110, A1, B2, 2); 
    vupdate_lane_int32(C111, A1, B2, 3); 
    vupdate_lane_int32(C112, A1, B3, 0); 
    vupdate_lane_int32(C113, A1, B3, 1); 
    vupdate_lane_int32(C114, A1, B3, 2); 
    vupdate_lane_int32(C115, A1, B3, 3); 
    bA+=8;
    bB+=16;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(0,12), C012); 
  vstoreC_int32(&Crref(0,13), C013); 
  vstoreC_int32(&Crref(0,14), C014); 
  vstoreC_int32(&Crref(0,15), C015); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(4,8), C18); 
  vstoreC_int32(&Crref(4,9), C19); 
  vstoreC_int32(&Crref(4,10), C110); 
  vstoreC_int32(&Crref(4,11), C111); 
  vstoreC_int32(&Crref(4,12), C112); 
  vstoreC_int32(&Crref(4,13), C113); 
  vstoreC_int32(&Crref(4,14), C114); 
  vstoreC_int32(&Crref(4,15), C115); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_8x20_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=8;
  const int NR=20;
  int32_t Ctmp[8*20];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  B0,  B1,  B2,  B3,  B4;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C012,  C013,  C014,  C015,  C016,  C017,  C018,  C019,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C18,  C19,  C110,  C111,  C112,  C113,  C114,  C115,  C116,  C117,  C118,  C119;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C012, 0);
    vinit_int32(C013, 0);
    vinit_int32(C014, 0);
    vinit_int32(C015, 0);
    vinit_int32(C016, 0);
    vinit_int32(C017, 0);
    vinit_int32(C018, 0);
    vinit_int32(C019, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C18, 0);
    vinit_int32(C19, 0);
    vinit_int32(C110, 0);
    vinit_int32(C111, 0);
    vinit_int32(C112, 0);
    vinit_int32(C113, 0);
    vinit_int32(C114, 0);
    vinit_int32(C115, 0);
    vinit_int32(C116, 0);
    vinit_int32(C117, 0);
    vinit_int32(C118, 0);
    vinit_int32(C119, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C012, &Crref(0, 12));
    vloadC_int32(C013, &Crref(0, 13));
    vloadC_int32(C014, &Crref(0, 14));
    vloadC_int32(C015, &Crref(0, 15));
    vloadC_int32(C016, &Crref(0, 16));
    vloadC_int32(C017, &Crref(0, 17));
    vloadC_int32(C018, &Crref(0, 18));
    vloadC_int32(C019, &Crref(0, 19));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C18, &Crref(4, 8));
    vloadC_int32(C19, &Crref(4, 9));
    vloadC_int32(C110, &Crref(4, 10));
    vloadC_int32(C111, &Crref(4, 11));
    vloadC_int32(C112, &Crref(4, 12));
    vloadC_int32(C113, &Crref(4, 13));
    vloadC_int32(C114, &Crref(4, 14));
    vloadC_int32(C115, &Crref(4, 15));
    vloadC_int32(C116, &Crref(4, 16));
    vloadC_int32(C117, &Crref(4, 17));
    vloadC_int32(C118, &Crref(4, 18));
    vloadC_int32(C119, &Crref(4, 19));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vload_int32(B3, &Br[bB + 12]);
    vload_int32(B4, &Br[bB + 16]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C012, A0, B3, 0); 
    vupdate_lane_int32(C013, A0, B3, 1); 
    vupdate_lane_int32(C014, A0, B3, 2); 
    vupdate_lane_int32(C015, A0, B3, 3); 
    vupdate_lane_int32(C016, A0, B4, 0); 
    vupdate_lane_int32(C017, A0, B4, 1); 
    vupdate_lane_int32(C018, A0, B4, 2); 
    vupdate_lane_int32(C019, A0, B4, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C18, A1, B2, 0); 
    vupdate_lane_int32(C19, A1, B2, 1); 
    vupdate_lane_int32(C110, A1, B2, 2); 
    vupdate_lane_int32(C111, A1, B2, 3); 
    vupdate_lane_int32(C112, A1, B3, 0); 
    vupdate_lane_int32(C113, A1, B3, 1); 
    vupdate_lane_int32(C114, A1, B3, 2); 
    vupdate_lane_int32(C115, A1, B3, 3); 
    vupdate_lane_int32(C116, A1, B4, 0); 
    vupdate_lane_int32(C117, A1, B4, 1); 
    vupdate_lane_int32(C118, A1, B4, 2); 
    vupdate_lane_int32(C119, A1, B4, 3); 
    bA+=8;
    bB+=20;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(0,12), C012); 
  vstoreC_int32(&Crref(0,13), C013); 
  vstoreC_int32(&Crref(0,14), C014); 
  vstoreC_int32(&Crref(0,15), C015); 
  vstoreC_int32(&Crref(0,16), C016); 
  vstoreC_int32(&Crref(0,17), C017); 
  vstoreC_int32(&Crref(0,18), C018); 
  vstoreC_int32(&Crref(0,19), C019); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(4,8), C18); 
  vstoreC_int32(&Crref(4,9), C19); 
  vstoreC_int32(&Crref(4,10), C110); 
  vstoreC_int32(&Crref(4,11), C111); 
  vstoreC_int32(&Crref(4,12), C112); 
  vstoreC_int32(&Crref(4,13), C113); 
  vstoreC_int32(&Crref(4,14), C114); 
  vstoreC_int32(&Crref(4,15), C115); 
  vstoreC_int32(&Crref(4,16), C116); 
  vstoreC_int32(&Crref(4,17), C117); 
  vstoreC_int32(&Crref(4,18), C118); 
  vstoreC_int32(&Crref(4,19), C119); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_12x4_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=12;
  const int NR=4;
  int32_t Ctmp[12*4];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  B0;
  int32x4_t  C00,  C01,  C02,  C03,  C10,  C11,  C12,  C13,  C20,  C21,  C22,  C23;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(B0, &Br[bB + 0]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    bA+=12;
    bB+=4;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_12x8_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=12;
  const int NR=8;
  int32_t Ctmp[12*8];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  B0,  B1;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C20,  C21,  C22,  C23,  C24,  C25,  C26,  C27;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C24, 0);
    vinit_int32(C25, 0);
    vinit_int32(C26, 0);
    vinit_int32(C27, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C24, &Crref(8, 4));
    vloadC_int32(C25, &Crref(8, 5));
    vloadC_int32(C26, &Crref(8, 6));
    vloadC_int32(C27, &Crref(8, 7));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C24, A2, B1, 0); 
    vupdate_lane_int32(C25, A2, B1, 1); 
    vupdate_lane_int32(C26, A2, B1, 2); 
    vupdate_lane_int32(C27, A2, B1, 3); 
    bA+=12;
    bB+=8;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(8,4), C24); 
  vstoreC_int32(&Crref(8,5), C25); 
  vstoreC_int32(&Crref(8,6), C26); 
  vstoreC_int32(&Crref(8,7), C27); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_12x12_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=12;
  const int NR=12;
  int32_t Ctmp[12*12];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  B0,  B1,  B2;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C18,  C19,  C110,  C111,  C20,  C21,  C22,  C23,  C24,  C25,  C26,  C27,  C28,  C29,  C210,  C211;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C18, 0);
    vinit_int32(C19, 0);
    vinit_int32(C110, 0);
    vinit_int32(C111, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C24, 0);
    vinit_int32(C25, 0);
    vinit_int32(C26, 0);
    vinit_int32(C27, 0);
    vinit_int32(C28, 0);
    vinit_int32(C29, 0);
    vinit_int32(C210, 0);
    vinit_int32(C211, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C18, &Crref(4, 8));
    vloadC_int32(C19, &Crref(4, 9));
    vloadC_int32(C110, &Crref(4, 10));
    vloadC_int32(C111, &Crref(4, 11));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C24, &Crref(8, 4));
    vloadC_int32(C25, &Crref(8, 5));
    vloadC_int32(C26, &Crref(8, 6));
    vloadC_int32(C27, &Crref(8, 7));
    vloadC_int32(C28, &Crref(8, 8));
    vloadC_int32(C29, &Crref(8, 9));
    vloadC_int32(C210, &Crref(8, 10));
    vloadC_int32(C211, &Crref(8, 11));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C18, A1, B2, 0); 
    vupdate_lane_int32(C19, A1, B2, 1); 
    vupdate_lane_int32(C110, A1, B2, 2); 
    vupdate_lane_int32(C111, A1, B2, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C24, A2, B1, 0); 
    vupdate_lane_int32(C25, A2, B1, 1); 
    vupdate_lane_int32(C26, A2, B1, 2); 
    vupdate_lane_int32(C27, A2, B1, 3); 
    vupdate_lane_int32(C28, A2, B2, 0); 
    vupdate_lane_int32(C29, A2, B2, 1); 
    vupdate_lane_int32(C210, A2, B2, 2); 
    vupdate_lane_int32(C211, A2, B2, 3); 
    bA+=12;
    bB+=12;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(4,8), C18); 
  vstoreC_int32(&Crref(4,9), C19); 
  vstoreC_int32(&Crref(4,10), C110); 
  vstoreC_int32(&Crref(4,11), C111); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(8,4), C24); 
  vstoreC_int32(&Crref(8,5), C25); 
  vstoreC_int32(&Crref(8,6), C26); 
  vstoreC_int32(&Crref(8,7), C27); 
  vstoreC_int32(&Crref(8,8), C28); 
  vstoreC_int32(&Crref(8,9), C29); 
  vstoreC_int32(&Crref(8,10), C210); 
  vstoreC_int32(&Crref(8,11), C211); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_12x16_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=12;
  const int NR=16;
  int32_t Ctmp[12*16];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  B0,  B1,  B2,  B3;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C012,  C013,  C014,  C015,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C18,  C19,  C110,  C111,  C112,  C113,  C114,  C115,  C20,  C21,  C22,  C23,  C24,  C25,  C26,  C27,  C28,  C29,  C210,  C211,  C212,  C213,  C214,  C215;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C012, 0);
    vinit_int32(C013, 0);
    vinit_int32(C014, 0);
    vinit_int32(C015, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C18, 0);
    vinit_int32(C19, 0);
    vinit_int32(C110, 0);
    vinit_int32(C111, 0);
    vinit_int32(C112, 0);
    vinit_int32(C113, 0);
    vinit_int32(C114, 0);
    vinit_int32(C115, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C24, 0);
    vinit_int32(C25, 0);
    vinit_int32(C26, 0);
    vinit_int32(C27, 0);
    vinit_int32(C28, 0);
    vinit_int32(C29, 0);
    vinit_int32(C210, 0);
    vinit_int32(C211, 0);
    vinit_int32(C212, 0);
    vinit_int32(C213, 0);
    vinit_int32(C214, 0);
    vinit_int32(C215, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C012, &Crref(0, 12));
    vloadC_int32(C013, &Crref(0, 13));
    vloadC_int32(C014, &Crref(0, 14));
    vloadC_int32(C015, &Crref(0, 15));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C18, &Crref(4, 8));
    vloadC_int32(C19, &Crref(4, 9));
    vloadC_int32(C110, &Crref(4, 10));
    vloadC_int32(C111, &Crref(4, 11));
    vloadC_int32(C112, &Crref(4, 12));
    vloadC_int32(C113, &Crref(4, 13));
    vloadC_int32(C114, &Crref(4, 14));
    vloadC_int32(C115, &Crref(4, 15));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C24, &Crref(8, 4));
    vloadC_int32(C25, &Crref(8, 5));
    vloadC_int32(C26, &Crref(8, 6));
    vloadC_int32(C27, &Crref(8, 7));
    vloadC_int32(C28, &Crref(8, 8));
    vloadC_int32(C29, &Crref(8, 9));
    vloadC_int32(C210, &Crref(8, 10));
    vloadC_int32(C211, &Crref(8, 11));
    vloadC_int32(C212, &Crref(8, 12));
    vloadC_int32(C213, &Crref(8, 13));
    vloadC_int32(C214, &Crref(8, 14));
    vloadC_int32(C215, &Crref(8, 15));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vload_int32(B3, &Br[bB + 12]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C012, A0, B3, 0); 
    vupdate_lane_int32(C013, A0, B3, 1); 
    vupdate_lane_int32(C014, A0, B3, 2); 
    vupdate_lane_int32(C015, A0, B3, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C18, A1, B2, 0); 
    vupdate_lane_int32(C19, A1, B2, 1); 
    vupdate_lane_int32(C110, A1, B2, 2); 
    vupdate_lane_int32(C111, A1, B2, 3); 
    vupdate_lane_int32(C112, A1, B3, 0); 
    vupdate_lane_int32(C113, A1, B3, 1); 
    vupdate_lane_int32(C114, A1, B3, 2); 
    vupdate_lane_int32(C115, A1, B3, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C24, A2, B1, 0); 
    vupdate_lane_int32(C25, A2, B1, 1); 
    vupdate_lane_int32(C26, A2, B1, 2); 
    vupdate_lane_int32(C27, A2, B1, 3); 
    vupdate_lane_int32(C28, A2, B2, 0); 
    vupdate_lane_int32(C29, A2, B2, 1); 
    vupdate_lane_int32(C210, A2, B2, 2); 
    vupdate_lane_int32(C211, A2, B2, 3); 
    vupdate_lane_int32(C212, A2, B3, 0); 
    vupdate_lane_int32(C213, A2, B3, 1); 
    vupdate_lane_int32(C214, A2, B3, 2); 
    vupdate_lane_int32(C215, A2, B3, 3); 
    bA+=12;
    bB+=16;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(0,12), C012); 
  vstoreC_int32(&Crref(0,13), C013); 
  vstoreC_int32(&Crref(0,14), C014); 
  vstoreC_int32(&Crref(0,15), C015); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(4,8), C18); 
  vstoreC_int32(&Crref(4,9), C19); 
  vstoreC_int32(&Crref(4,10), C110); 
  vstoreC_int32(&Crref(4,11), C111); 
  vstoreC_int32(&Crref(4,12), C112); 
  vstoreC_int32(&Crref(4,13), C113); 
  vstoreC_int32(&Crref(4,14), C114); 
  vstoreC_int32(&Crref(4,15), C115); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(8,4), C24); 
  vstoreC_int32(&Crref(8,5), C25); 
  vstoreC_int32(&Crref(8,6), C26); 
  vstoreC_int32(&Crref(8,7), C27); 
  vstoreC_int32(&Crref(8,8), C28); 
  vstoreC_int32(&Crref(8,9), C29); 
  vstoreC_int32(&Crref(8,10), C210); 
  vstoreC_int32(&Crref(8,11), C211); 
  vstoreC_int32(&Crref(8,12), C212); 
  vstoreC_int32(&Crref(8,13), C213); 
  vstoreC_int32(&Crref(8,14), C214); 
  vstoreC_int32(&Crref(8,15), C215); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_12x20_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=12;
  const int NR=20;
  int32_t Ctmp[12*20];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  B0,  B1,  B2,  B3,  B4;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C012,  C013,  C014,  C015,  C016,  C017,  C018,  C019,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C18,  C19,  C110,  C111,  C112,  C113,  C114,  C115,  C116,  C117,  C118,  C119,  C20,  C21,  C22,  C23,  C24,  C25,  C26,  C27,  C28,  C29,  C210,  C211,  C212,  C213,  C214,  C215,  C216,  C217,  C218,  C219;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C012, 0);
    vinit_int32(C013, 0);
    vinit_int32(C014, 0);
    vinit_int32(C015, 0);
    vinit_int32(C016, 0);
    vinit_int32(C017, 0);
    vinit_int32(C018, 0);
    vinit_int32(C019, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C18, 0);
    vinit_int32(C19, 0);
    vinit_int32(C110, 0);
    vinit_int32(C111, 0);
    vinit_int32(C112, 0);
    vinit_int32(C113, 0);
    vinit_int32(C114, 0);
    vinit_int32(C115, 0);
    vinit_int32(C116, 0);
    vinit_int32(C117, 0);
    vinit_int32(C118, 0);
    vinit_int32(C119, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C24, 0);
    vinit_int32(C25, 0);
    vinit_int32(C26, 0);
    vinit_int32(C27, 0);
    vinit_int32(C28, 0);
    vinit_int32(C29, 0);
    vinit_int32(C210, 0);
    vinit_int32(C211, 0);
    vinit_int32(C212, 0);
    vinit_int32(C213, 0);
    vinit_int32(C214, 0);
    vinit_int32(C215, 0);
    vinit_int32(C216, 0);
    vinit_int32(C217, 0);
    vinit_int32(C218, 0);
    vinit_int32(C219, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C012, &Crref(0, 12));
    vloadC_int32(C013, &Crref(0, 13));
    vloadC_int32(C014, &Crref(0, 14));
    vloadC_int32(C015, &Crref(0, 15));
    vloadC_int32(C016, &Crref(0, 16));
    vloadC_int32(C017, &Crref(0, 17));
    vloadC_int32(C018, &Crref(0, 18));
    vloadC_int32(C019, &Crref(0, 19));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C18, &Crref(4, 8));
    vloadC_int32(C19, &Crref(4, 9));
    vloadC_int32(C110, &Crref(4, 10));
    vloadC_int32(C111, &Crref(4, 11));
    vloadC_int32(C112, &Crref(4, 12));
    vloadC_int32(C113, &Crref(4, 13));
    vloadC_int32(C114, &Crref(4, 14));
    vloadC_int32(C115, &Crref(4, 15));
    vloadC_int32(C116, &Crref(4, 16));
    vloadC_int32(C117, &Crref(4, 17));
    vloadC_int32(C118, &Crref(4, 18));
    vloadC_int32(C119, &Crref(4, 19));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C24, &Crref(8, 4));
    vloadC_int32(C25, &Crref(8, 5));
    vloadC_int32(C26, &Crref(8, 6));
    vloadC_int32(C27, &Crref(8, 7));
    vloadC_int32(C28, &Crref(8, 8));
    vloadC_int32(C29, &Crref(8, 9));
    vloadC_int32(C210, &Crref(8, 10));
    vloadC_int32(C211, &Crref(8, 11));
    vloadC_int32(C212, &Crref(8, 12));
    vloadC_int32(C213, &Crref(8, 13));
    vloadC_int32(C214, &Crref(8, 14));
    vloadC_int32(C215, &Crref(8, 15));
    vloadC_int32(C216, &Crref(8, 16));
    vloadC_int32(C217, &Crref(8, 17));
    vloadC_int32(C218, &Crref(8, 18));
    vloadC_int32(C219, &Crref(8, 19));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vload_int32(B3, &Br[bB + 12]);
    vload_int32(B4, &Br[bB + 16]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C012, A0, B3, 0); 
    vupdate_lane_int32(C013, A0, B3, 1); 
    vupdate_lane_int32(C014, A0, B3, 2); 
    vupdate_lane_int32(C015, A0, B3, 3); 
    vupdate_lane_int32(C016, A0, B4, 0); 
    vupdate_lane_int32(C017, A0, B4, 1); 
    vupdate_lane_int32(C018, A0, B4, 2); 
    vupdate_lane_int32(C019, A0, B4, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C18, A1, B2, 0); 
    vupdate_lane_int32(C19, A1, B2, 1); 
    vupdate_lane_int32(C110, A1, B2, 2); 
    vupdate_lane_int32(C111, A1, B2, 3); 
    vupdate_lane_int32(C112, A1, B3, 0); 
    vupdate_lane_int32(C113, A1, B3, 1); 
    vupdate_lane_int32(C114, A1, B3, 2); 
    vupdate_lane_int32(C115, A1, B3, 3); 
    vupdate_lane_int32(C116, A1, B4, 0); 
    vupdate_lane_int32(C117, A1, B4, 1); 
    vupdate_lane_int32(C118, A1, B4, 2); 
    vupdate_lane_int32(C119, A1, B4, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C24, A2, B1, 0); 
    vupdate_lane_int32(C25, A2, B1, 1); 
    vupdate_lane_int32(C26, A2, B1, 2); 
    vupdate_lane_int32(C27, A2, B1, 3); 
    vupdate_lane_int32(C28, A2, B2, 0); 
    vupdate_lane_int32(C29, A2, B2, 1); 
    vupdate_lane_int32(C210, A2, B2, 2); 
    vupdate_lane_int32(C211, A2, B2, 3); 
    vupdate_lane_int32(C212, A2, B3, 0); 
    vupdate_lane_int32(C213, A2, B3, 1); 
    vupdate_lane_int32(C214, A2, B3, 2); 
    vupdate_lane_int32(C215, A2, B3, 3); 
    vupdate_lane_int32(C216, A2, B4, 0); 
    vupdate_lane_int32(C217, A2, B4, 1); 
    vupdate_lane_int32(C218, A2, B4, 2); 
    vupdate_lane_int32(C219, A2, B4, 3); 
    bA+=12;
    bB+=20;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(0,12), C012); 
  vstoreC_int32(&Crref(0,13), C013); 
  vstoreC_int32(&Crref(0,14), C014); 
  vstoreC_int32(&Crref(0,15), C015); 
  vstoreC_int32(&Crref(0,16), C016); 
  vstoreC_int32(&Crref(0,17), C017); 
  vstoreC_int32(&Crref(0,18), C018); 
  vstoreC_int32(&Crref(0,19), C019); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(4,8), C18); 
  vstoreC_int32(&Crref(4,9), C19); 
  vstoreC_int32(&Crref(4,10), C110); 
  vstoreC_int32(&Crref(4,11), C111); 
  vstoreC_int32(&Crref(4,12), C112); 
  vstoreC_int32(&Crref(4,13), C113); 
  vstoreC_int32(&Crref(4,14), C114); 
  vstoreC_int32(&Crref(4,15), C115); 
  vstoreC_int32(&Crref(4,16), C116); 
  vstoreC_int32(&Crref(4,17), C117); 
  vstoreC_int32(&Crref(4,18), C118); 
  vstoreC_int32(&Crref(4,19), C119); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(8,4), C24); 
  vstoreC_int32(&Crref(8,5), C25); 
  vstoreC_int32(&Crref(8,6), C26); 
  vstoreC_int32(&Crref(8,7), C27); 
  vstoreC_int32(&Crref(8,8), C28); 
  vstoreC_int32(&Crref(8,9), C29); 
  vstoreC_int32(&Crref(8,10), C210); 
  vstoreC_int32(&Crref(8,11), C211); 
  vstoreC_int32(&Crref(8,12), C212); 
  vstoreC_int32(&Crref(8,13), C213); 
  vstoreC_int32(&Crref(8,14), C214); 
  vstoreC_int32(&Crref(8,15), C215); 
  vstoreC_int32(&Crref(8,16), C216); 
  vstoreC_int32(&Crref(8,17), C217); 
  vstoreC_int32(&Crref(8,18), C218); 
  vstoreC_int32(&Crref(8,19), C219); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_16x4_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=16;
  const int NR=4;
  int32_t Ctmp[16*4];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  A3,  B0;
  int32x4_t  C00,  C01,  C02,  C03,  C10,  C11,  C12,  C13,  C20,  C21,  C22,  C23,  C30,  C31,  C32,  C33;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C30, 0);
    vinit_int32(C31, 0);
    vinit_int32(C32, 0);
    vinit_int32(C33, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C30, &Crref(12, 0));
    vloadC_int32(C31, &Crref(12, 1));
    vloadC_int32(C32, &Crref(12, 2));
    vloadC_int32(C33, &Crref(12, 3));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(A3, &Ar[bA + 12]);
    vload_int32(B0, &Br[bB + 0]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C30, A3, B0, 0); 
    vupdate_lane_int32(C31, A3, B0, 1); 
    vupdate_lane_int32(C32, A3, B0, 2); 
    vupdate_lane_int32(C33, A3, B0, 3); 
    bA+=16;
    bB+=4;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(12,0), C30); 
  vstoreC_int32(&Crref(12,1), C31); 
  vstoreC_int32(&Crref(12,2), C32); 
  vstoreC_int32(&Crref(12,3), C33); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_16x8_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=16;
  const int NR=8;
  int32_t Ctmp[16*8];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  A3,  B0,  B1;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C20,  C21,  C22,  C23,  C24,  C25,  C26,  C27,  C30,  C31,  C32,  C33,  C34,  C35,  C36,  C37;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C24, 0);
    vinit_int32(C25, 0);
    vinit_int32(C26, 0);
    vinit_int32(C27, 0);
    vinit_int32(C30, 0);
    vinit_int32(C31, 0);
    vinit_int32(C32, 0);
    vinit_int32(C33, 0);
    vinit_int32(C34, 0);
    vinit_int32(C35, 0);
    vinit_int32(C36, 0);
    vinit_int32(C37, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C24, &Crref(8, 4));
    vloadC_int32(C25, &Crref(8, 5));
    vloadC_int32(C26, &Crref(8, 6));
    vloadC_int32(C27, &Crref(8, 7));
    vloadC_int32(C30, &Crref(12, 0));
    vloadC_int32(C31, &Crref(12, 1));
    vloadC_int32(C32, &Crref(12, 2));
    vloadC_int32(C33, &Crref(12, 3));
    vloadC_int32(C34, &Crref(12, 4));
    vloadC_int32(C35, &Crref(12, 5));
    vloadC_int32(C36, &Crref(12, 6));
    vloadC_int32(C37, &Crref(12, 7));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(A3, &Ar[bA + 12]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C24, A2, B1, 0); 
    vupdate_lane_int32(C25, A2, B1, 1); 
    vupdate_lane_int32(C26, A2, B1, 2); 
    vupdate_lane_int32(C27, A2, B1, 3); 
    vupdate_lane_int32(C30, A3, B0, 0); 
    vupdate_lane_int32(C31, A3, B0, 1); 
    vupdate_lane_int32(C32, A3, B0, 2); 
    vupdate_lane_int32(C33, A3, B0, 3); 
    vupdate_lane_int32(C34, A3, B1, 0); 
    vupdate_lane_int32(C35, A3, B1, 1); 
    vupdate_lane_int32(C36, A3, B1, 2); 
    vupdate_lane_int32(C37, A3, B1, 3); 
    bA+=16;
    bB+=8;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(8,4), C24); 
  vstoreC_int32(&Crref(8,5), C25); 
  vstoreC_int32(&Crref(8,6), C26); 
  vstoreC_int32(&Crref(8,7), C27); 
  vstoreC_int32(&Crref(12,0), C30); 
  vstoreC_int32(&Crref(12,1), C31); 
  vstoreC_int32(&Crref(12,2), C32); 
  vstoreC_int32(&Crref(12,3), C33); 
  vstoreC_int32(&Crref(12,4), C34); 
  vstoreC_int32(&Crref(12,5), C35); 
  vstoreC_int32(&Crref(12,6), C36); 
  vstoreC_int32(&Crref(12,7), C37); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_16x12_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=16;
  const int NR=12;
  int32_t Ctmp[16*12];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  A3,  B0,  B1,  B2;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C18,  C19,  C110,  C111,  C20,  C21,  C22,  C23,  C24,  C25,  C26,  C27,  C28,  C29,  C210,  C211,  C30,  C31,  C32,  C33,  C34,  C35,  C36,  C37,  C38,  C39,  C310,  C311;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C18, 0);
    vinit_int32(C19, 0);
    vinit_int32(C110, 0);
    vinit_int32(C111, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C24, 0);
    vinit_int32(C25, 0);
    vinit_int32(C26, 0);
    vinit_int32(C27, 0);
    vinit_int32(C28, 0);
    vinit_int32(C29, 0);
    vinit_int32(C210, 0);
    vinit_int32(C211, 0);
    vinit_int32(C30, 0);
    vinit_int32(C31, 0);
    vinit_int32(C32, 0);
    vinit_int32(C33, 0);
    vinit_int32(C34, 0);
    vinit_int32(C35, 0);
    vinit_int32(C36, 0);
    vinit_int32(C37, 0);
    vinit_int32(C38, 0);
    vinit_int32(C39, 0);
    vinit_int32(C310, 0);
    vinit_int32(C311, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C18, &Crref(4, 8));
    vloadC_int32(C19, &Crref(4, 9));
    vloadC_int32(C110, &Crref(4, 10));
    vloadC_int32(C111, &Crref(4, 11));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C24, &Crref(8, 4));
    vloadC_int32(C25, &Crref(8, 5));
    vloadC_int32(C26, &Crref(8, 6));
    vloadC_int32(C27, &Crref(8, 7));
    vloadC_int32(C28, &Crref(8, 8));
    vloadC_int32(C29, &Crref(8, 9));
    vloadC_int32(C210, &Crref(8, 10));
    vloadC_int32(C211, &Crref(8, 11));
    vloadC_int32(C30, &Crref(12, 0));
    vloadC_int32(C31, &Crref(12, 1));
    vloadC_int32(C32, &Crref(12, 2));
    vloadC_int32(C33, &Crref(12, 3));
    vloadC_int32(C34, &Crref(12, 4));
    vloadC_int32(C35, &Crref(12, 5));
    vloadC_int32(C36, &Crref(12, 6));
    vloadC_int32(C37, &Crref(12, 7));
    vloadC_int32(C38, &Crref(12, 8));
    vloadC_int32(C39, &Crref(12, 9));
    vloadC_int32(C310, &Crref(12, 10));
    vloadC_int32(C311, &Crref(12, 11));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(A3, &Ar[bA + 12]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C18, A1, B2, 0); 
    vupdate_lane_int32(C19, A1, B2, 1); 
    vupdate_lane_int32(C110, A1, B2, 2); 
    vupdate_lane_int32(C111, A1, B2, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C24, A2, B1, 0); 
    vupdate_lane_int32(C25, A2, B1, 1); 
    vupdate_lane_int32(C26, A2, B1, 2); 
    vupdate_lane_int32(C27, A2, B1, 3); 
    vupdate_lane_int32(C28, A2, B2, 0); 
    vupdate_lane_int32(C29, A2, B2, 1); 
    vupdate_lane_int32(C210, A2, B2, 2); 
    vupdate_lane_int32(C211, A2, B2, 3); 
    vupdate_lane_int32(C30, A3, B0, 0); 
    vupdate_lane_int32(C31, A3, B0, 1); 
    vupdate_lane_int32(C32, A3, B0, 2); 
    vupdate_lane_int32(C33, A3, B0, 3); 
    vupdate_lane_int32(C34, A3, B1, 0); 
    vupdate_lane_int32(C35, A3, B1, 1); 
    vupdate_lane_int32(C36, A3, B1, 2); 
    vupdate_lane_int32(C37, A3, B1, 3); 
    vupdate_lane_int32(C38, A3, B2, 0); 
    vupdate_lane_int32(C39, A3, B2, 1); 
    vupdate_lane_int32(C310, A3, B2, 2); 
    vupdate_lane_int32(C311, A3, B2, 3); 
    bA+=16;
    bB+=12;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(4,8), C18); 
  vstoreC_int32(&Crref(4,9), C19); 
  vstoreC_int32(&Crref(4,10), C110); 
  vstoreC_int32(&Crref(4,11), C111); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(8,4), C24); 
  vstoreC_int32(&Crref(8,5), C25); 
  vstoreC_int32(&Crref(8,6), C26); 
  vstoreC_int32(&Crref(8,7), C27); 
  vstoreC_int32(&Crref(8,8), C28); 
  vstoreC_int32(&Crref(8,9), C29); 
  vstoreC_int32(&Crref(8,10), C210); 
  vstoreC_int32(&Crref(8,11), C211); 
  vstoreC_int32(&Crref(12,0), C30); 
  vstoreC_int32(&Crref(12,1), C31); 
  vstoreC_int32(&Crref(12,2), C32); 
  vstoreC_int32(&Crref(12,3), C33); 
  vstoreC_int32(&Crref(12,4), C34); 
  vstoreC_int32(&Crref(12,5), C35); 
  vstoreC_int32(&Crref(12,6), C36); 
  vstoreC_int32(&Crref(12,7), C37); 
  vstoreC_int32(&Crref(12,8), C38); 
  vstoreC_int32(&Crref(12,9), C39); 
  vstoreC_int32(&Crref(12,10), C310); 
  vstoreC_int32(&Crref(12,11), C311); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_16x16_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=16;
  const int NR=16;
  int32_t Ctmp[16*16];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  A3,  B0,  B1,  B2,  B3;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C012,  C013,  C014,  C015,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C18,  C19,  C110,  C111,  C112,  C113,  C114,  C115,  C20,  C21,  C22,  C23,  C24,  C25,  C26,  C27,  C28,  C29,  C210,  C211,  C212,  C213,  C214,  C215,  C30,  C31,  C32,  C33,  C34,  C35,  C36,  C37,  C38,  C39,  C310,  C311,  C312,  C313,  C314,  C315;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C012, 0);
    vinit_int32(C013, 0);
    vinit_int32(C014, 0);
    vinit_int32(C015, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C18, 0);
    vinit_int32(C19, 0);
    vinit_int32(C110, 0);
    vinit_int32(C111, 0);
    vinit_int32(C112, 0);
    vinit_int32(C113, 0);
    vinit_int32(C114, 0);
    vinit_int32(C115, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C24, 0);
    vinit_int32(C25, 0);
    vinit_int32(C26, 0);
    vinit_int32(C27, 0);
    vinit_int32(C28, 0);
    vinit_int32(C29, 0);
    vinit_int32(C210, 0);
    vinit_int32(C211, 0);
    vinit_int32(C212, 0);
    vinit_int32(C213, 0);
    vinit_int32(C214, 0);
    vinit_int32(C215, 0);
    vinit_int32(C30, 0);
    vinit_int32(C31, 0);
    vinit_int32(C32, 0);
    vinit_int32(C33, 0);
    vinit_int32(C34, 0);
    vinit_int32(C35, 0);
    vinit_int32(C36, 0);
    vinit_int32(C37, 0);
    vinit_int32(C38, 0);
    vinit_int32(C39, 0);
    vinit_int32(C310, 0);
    vinit_int32(C311, 0);
    vinit_int32(C312, 0);
    vinit_int32(C313, 0);
    vinit_int32(C314, 0);
    vinit_int32(C315, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C012, &Crref(0, 12));
    vloadC_int32(C013, &Crref(0, 13));
    vloadC_int32(C014, &Crref(0, 14));
    vloadC_int32(C015, &Crref(0, 15));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C18, &Crref(4, 8));
    vloadC_int32(C19, &Crref(4, 9));
    vloadC_int32(C110, &Crref(4, 10));
    vloadC_int32(C111, &Crref(4, 11));
    vloadC_int32(C112, &Crref(4, 12));
    vloadC_int32(C113, &Crref(4, 13));
    vloadC_int32(C114, &Crref(4, 14));
    vloadC_int32(C115, &Crref(4, 15));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C24, &Crref(8, 4));
    vloadC_int32(C25, &Crref(8, 5));
    vloadC_int32(C26, &Crref(8, 6));
    vloadC_int32(C27, &Crref(8, 7));
    vloadC_int32(C28, &Crref(8, 8));
    vloadC_int32(C29, &Crref(8, 9));
    vloadC_int32(C210, &Crref(8, 10));
    vloadC_int32(C211, &Crref(8, 11));
    vloadC_int32(C212, &Crref(8, 12));
    vloadC_int32(C213, &Crref(8, 13));
    vloadC_int32(C214, &Crref(8, 14));
    vloadC_int32(C215, &Crref(8, 15));
    vloadC_int32(C30, &Crref(12, 0));
    vloadC_int32(C31, &Crref(12, 1));
    vloadC_int32(C32, &Crref(12, 2));
    vloadC_int32(C33, &Crref(12, 3));
    vloadC_int32(C34, &Crref(12, 4));
    vloadC_int32(C35, &Crref(12, 5));
    vloadC_int32(C36, &Crref(12, 6));
    vloadC_int32(C37, &Crref(12, 7));
    vloadC_int32(C38, &Crref(12, 8));
    vloadC_int32(C39, &Crref(12, 9));
    vloadC_int32(C310, &Crref(12, 10));
    vloadC_int32(C311, &Crref(12, 11));
    vloadC_int32(C312, &Crref(12, 12));
    vloadC_int32(C313, &Crref(12, 13));
    vloadC_int32(C314, &Crref(12, 14));
    vloadC_int32(C315, &Crref(12, 15));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(A3, &Ar[bA + 12]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vload_int32(B3, &Br[bB + 12]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C012, A0, B3, 0); 
    vupdate_lane_int32(C013, A0, B3, 1); 
    vupdate_lane_int32(C014, A0, B3, 2); 
    vupdate_lane_int32(C015, A0, B3, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C18, A1, B2, 0); 
    vupdate_lane_int32(C19, A1, B2, 1); 
    vupdate_lane_int32(C110, A1, B2, 2); 
    vupdate_lane_int32(C111, A1, B2, 3); 
    vupdate_lane_int32(C112, A1, B3, 0); 
    vupdate_lane_int32(C113, A1, B3, 1); 
    vupdate_lane_int32(C114, A1, B3, 2); 
    vupdate_lane_int32(C115, A1, B3, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C24, A2, B1, 0); 
    vupdate_lane_int32(C25, A2, B1, 1); 
    vupdate_lane_int32(C26, A2, B1, 2); 
    vupdate_lane_int32(C27, A2, B1, 3); 
    vupdate_lane_int32(C28, A2, B2, 0); 
    vupdate_lane_int32(C29, A2, B2, 1); 
    vupdate_lane_int32(C210, A2, B2, 2); 
    vupdate_lane_int32(C211, A2, B2, 3); 
    vupdate_lane_int32(C212, A2, B3, 0); 
    vupdate_lane_int32(C213, A2, B3, 1); 
    vupdate_lane_int32(C214, A2, B3, 2); 
    vupdate_lane_int32(C215, A2, B3, 3); 
    vupdate_lane_int32(C30, A3, B0, 0); 
    vupdate_lane_int32(C31, A3, B0, 1); 
    vupdate_lane_int32(C32, A3, B0, 2); 
    vupdate_lane_int32(C33, A3, B0, 3); 
    vupdate_lane_int32(C34, A3, B1, 0); 
    vupdate_lane_int32(C35, A3, B1, 1); 
    vupdate_lane_int32(C36, A3, B1, 2); 
    vupdate_lane_int32(C37, A3, B1, 3); 
    vupdate_lane_int32(C38, A3, B2, 0); 
    vupdate_lane_int32(C39, A3, B2, 1); 
    vupdate_lane_int32(C310, A3, B2, 2); 
    vupdate_lane_int32(C311, A3, B2, 3); 
    vupdate_lane_int32(C312, A3, B3, 0); 
    vupdate_lane_int32(C313, A3, B3, 1); 
    vupdate_lane_int32(C314, A3, B3, 2); 
    vupdate_lane_int32(C315, A3, B3, 3); 
    bA+=16;
    bB+=16;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(0,12), C012); 
  vstoreC_int32(&Crref(0,13), C013); 
  vstoreC_int32(&Crref(0,14), C014); 
  vstoreC_int32(&Crref(0,15), C015); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(4,8), C18); 
  vstoreC_int32(&Crref(4,9), C19); 
  vstoreC_int32(&Crref(4,10), C110); 
  vstoreC_int32(&Crref(4,11), C111); 
  vstoreC_int32(&Crref(4,12), C112); 
  vstoreC_int32(&Crref(4,13), C113); 
  vstoreC_int32(&Crref(4,14), C114); 
  vstoreC_int32(&Crref(4,15), C115); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(8,4), C24); 
  vstoreC_int32(&Crref(8,5), C25); 
  vstoreC_int32(&Crref(8,6), C26); 
  vstoreC_int32(&Crref(8,7), C27); 
  vstoreC_int32(&Crref(8,8), C28); 
  vstoreC_int32(&Crref(8,9), C29); 
  vstoreC_int32(&Crref(8,10), C210); 
  vstoreC_int32(&Crref(8,11), C211); 
  vstoreC_int32(&Crref(8,12), C212); 
  vstoreC_int32(&Crref(8,13), C213); 
  vstoreC_int32(&Crref(8,14), C214); 
  vstoreC_int32(&Crref(8,15), C215); 
  vstoreC_int32(&Crref(12,0), C30); 
  vstoreC_int32(&Crref(12,1), C31); 
  vstoreC_int32(&Crref(12,2), C32); 
  vstoreC_int32(&Crref(12,3), C33); 
  vstoreC_int32(&Crref(12,4), C34); 
  vstoreC_int32(&Crref(12,5), C35); 
  vstoreC_int32(&Crref(12,6), C36); 
  vstoreC_int32(&Crref(12,7), C37); 
  vstoreC_int32(&Crref(12,8), C38); 
  vstoreC_int32(&Crref(12,9), C39); 
  vstoreC_int32(&Crref(12,10), C310); 
  vstoreC_int32(&Crref(12,11), C311); 
  vstoreC_int32(&Crref(12,12), C312); 
  vstoreC_int32(&Crref(12,13), C313); 
  vstoreC_int32(&Crref(12,14), C314); 
  vstoreC_int32(&Crref(12,15), C315); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_16x20_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=16;
  const int NR=20;
  int32_t Ctmp[16*20];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  A3,  B0,  B1,  B2,  B3,  B4;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C012,  C013,  C014,  C015,  C016,  C017,  C018,  C019,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C18,  C19,  C110,  C111,  C112,  C113,  C114,  C115,  C116,  C117,  C118,  C119,  C20,  C21,  C22,  C23,  C24,  C25,  C26,  C27,  C28,  C29,  C210,  C211,  C212,  C213,  C214,  C215,  C216,  C217,  C218,  C219,  C30,  C31,  C32,  C33,  C34,  C35,  C36,  C37,  C38,  C39,  C310,  C311,  C312,  C313,  C314,  C315,  C316,  C317,  C318,  C319;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C012, 0);
    vinit_int32(C013, 0);
    vinit_int32(C014, 0);
    vinit_int32(C015, 0);
    vinit_int32(C016, 0);
    vinit_int32(C017, 0);
    vinit_int32(C018, 0);
    vinit_int32(C019, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C18, 0);
    vinit_int32(C19, 0);
    vinit_int32(C110, 0);
    vinit_int32(C111, 0);
    vinit_int32(C112, 0);
    vinit_int32(C113, 0);
    vinit_int32(C114, 0);
    vinit_int32(C115, 0);
    vinit_int32(C116, 0);
    vinit_int32(C117, 0);
    vinit_int32(C118, 0);
    vinit_int32(C119, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C24, 0);
    vinit_int32(C25, 0);
    vinit_int32(C26, 0);
    vinit_int32(C27, 0);
    vinit_int32(C28, 0);
    vinit_int32(C29, 0);
    vinit_int32(C210, 0);
    vinit_int32(C211, 0);
    vinit_int32(C212, 0);
    vinit_int32(C213, 0);
    vinit_int32(C214, 0);
    vinit_int32(C215, 0);
    vinit_int32(C216, 0);
    vinit_int32(C217, 0);
    vinit_int32(C218, 0);
    vinit_int32(C219, 0);
    vinit_int32(C30, 0);
    vinit_int32(C31, 0);
    vinit_int32(C32, 0);
    vinit_int32(C33, 0);
    vinit_int32(C34, 0);
    vinit_int32(C35, 0);
    vinit_int32(C36, 0);
    vinit_int32(C37, 0);
    vinit_int32(C38, 0);
    vinit_int32(C39, 0);
    vinit_int32(C310, 0);
    vinit_int32(C311, 0);
    vinit_int32(C312, 0);
    vinit_int32(C313, 0);
    vinit_int32(C314, 0);
    vinit_int32(C315, 0);
    vinit_int32(C316, 0);
    vinit_int32(C317, 0);
    vinit_int32(C318, 0);
    vinit_int32(C319, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C012, &Crref(0, 12));
    vloadC_int32(C013, &Crref(0, 13));
    vloadC_int32(C014, &Crref(0, 14));
    vloadC_int32(C015, &Crref(0, 15));
    vloadC_int32(C016, &Crref(0, 16));
    vloadC_int32(C017, &Crref(0, 17));
    vloadC_int32(C018, &Crref(0, 18));
    vloadC_int32(C019, &Crref(0, 19));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C18, &Crref(4, 8));
    vloadC_int32(C19, &Crref(4, 9));
    vloadC_int32(C110, &Crref(4, 10));
    vloadC_int32(C111, &Crref(4, 11));
    vloadC_int32(C112, &Crref(4, 12));
    vloadC_int32(C113, &Crref(4, 13));
    vloadC_int32(C114, &Crref(4, 14));
    vloadC_int32(C115, &Crref(4, 15));
    vloadC_int32(C116, &Crref(4, 16));
    vloadC_int32(C117, &Crref(4, 17));
    vloadC_int32(C118, &Crref(4, 18));
    vloadC_int32(C119, &Crref(4, 19));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C24, &Crref(8, 4));
    vloadC_int32(C25, &Crref(8, 5));
    vloadC_int32(C26, &Crref(8, 6));
    vloadC_int32(C27, &Crref(8, 7));
    vloadC_int32(C28, &Crref(8, 8));
    vloadC_int32(C29, &Crref(8, 9));
    vloadC_int32(C210, &Crref(8, 10));
    vloadC_int32(C211, &Crref(8, 11));
    vloadC_int32(C212, &Crref(8, 12));
    vloadC_int32(C213, &Crref(8, 13));
    vloadC_int32(C214, &Crref(8, 14));
    vloadC_int32(C215, &Crref(8, 15));
    vloadC_int32(C216, &Crref(8, 16));
    vloadC_int32(C217, &Crref(8, 17));
    vloadC_int32(C218, &Crref(8, 18));
    vloadC_int32(C219, &Crref(8, 19));
    vloadC_int32(C30, &Crref(12, 0));
    vloadC_int32(C31, &Crref(12, 1));
    vloadC_int32(C32, &Crref(12, 2));
    vloadC_int32(C33, &Crref(12, 3));
    vloadC_int32(C34, &Crref(12, 4));
    vloadC_int32(C35, &Crref(12, 5));
    vloadC_int32(C36, &Crref(12, 6));
    vloadC_int32(C37, &Crref(12, 7));
    vloadC_int32(C38, &Crref(12, 8));
    vloadC_int32(C39, &Crref(12, 9));
    vloadC_int32(C310, &Crref(12, 10));
    vloadC_int32(C311, &Crref(12, 11));
    vloadC_int32(C312, &Crref(12, 12));
    vloadC_int32(C313, &Crref(12, 13));
    vloadC_int32(C314, &Crref(12, 14));
    vloadC_int32(C315, &Crref(12, 15));
    vloadC_int32(C316, &Crref(12, 16));
    vloadC_int32(C317, &Crref(12, 17));
    vloadC_int32(C318, &Crref(12, 18));
    vloadC_int32(C319, &Crref(12, 19));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(A3, &Ar[bA + 12]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vload_int32(B3, &Br[bB + 12]);
    vload_int32(B4, &Br[bB + 16]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C012, A0, B3, 0); 
    vupdate_lane_int32(C013, A0, B3, 1); 
    vupdate_lane_int32(C014, A0, B3, 2); 
    vupdate_lane_int32(C015, A0, B3, 3); 
    vupdate_lane_int32(C016, A0, B4, 0); 
    vupdate_lane_int32(C017, A0, B4, 1); 
    vupdate_lane_int32(C018, A0, B4, 2); 
    vupdate_lane_int32(C019, A0, B4, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C18, A1, B2, 0); 
    vupdate_lane_int32(C19, A1, B2, 1); 
    vupdate_lane_int32(C110, A1, B2, 2); 
    vupdate_lane_int32(C111, A1, B2, 3); 
    vupdate_lane_int32(C112, A1, B3, 0); 
    vupdate_lane_int32(C113, A1, B3, 1); 
    vupdate_lane_int32(C114, A1, B3, 2); 
    vupdate_lane_int32(C115, A1, B3, 3); 
    vupdate_lane_int32(C116, A1, B4, 0); 
    vupdate_lane_int32(C117, A1, B4, 1); 
    vupdate_lane_int32(C118, A1, B4, 2); 
    vupdate_lane_int32(C119, A1, B4, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C24, A2, B1, 0); 
    vupdate_lane_int32(C25, A2, B1, 1); 
    vupdate_lane_int32(C26, A2, B1, 2); 
    vupdate_lane_int32(C27, A2, B1, 3); 
    vupdate_lane_int32(C28, A2, B2, 0); 
    vupdate_lane_int32(C29, A2, B2, 1); 
    vupdate_lane_int32(C210, A2, B2, 2); 
    vupdate_lane_int32(C211, A2, B2, 3); 
    vupdate_lane_int32(C212, A2, B3, 0); 
    vupdate_lane_int32(C213, A2, B3, 1); 
    vupdate_lane_int32(C214, A2, B3, 2); 
    vupdate_lane_int32(C215, A2, B3, 3); 
    vupdate_lane_int32(C216, A2, B4, 0); 
    vupdate_lane_int32(C217, A2, B4, 1); 
    vupdate_lane_int32(C218, A2, B4, 2); 
    vupdate_lane_int32(C219, A2, B4, 3); 
    vupdate_lane_int32(C30, A3, B0, 0); 
    vupdate_lane_int32(C31, A3, B0, 1); 
    vupdate_lane_int32(C32, A3, B0, 2); 
    vupdate_lane_int32(C33, A3, B0, 3); 
    vupdate_lane_int32(C34, A3, B1, 0); 
    vupdate_lane_int32(C35, A3, B1, 1); 
    vupdate_lane_int32(C36, A3, B1, 2); 
    vupdate_lane_int32(C37, A3, B1, 3); 
    vupdate_lane_int32(C38, A3, B2, 0); 
    vupdate_lane_int32(C39, A3, B2, 1); 
    vupdate_lane_int32(C310, A3, B2, 2); 
    vupdate_lane_int32(C311, A3, B2, 3); 
    vupdate_lane_int32(C312, A3, B3, 0); 
    vupdate_lane_int32(C313, A3, B3, 1); 
    vupdate_lane_int32(C314, A3, B3, 2); 
    vupdate_lane_int32(C315, A3, B3, 3); 
    vupdate_lane_int32(C316, A3, B4, 0); 
    vupdate_lane_int32(C317, A3, B4, 1); 
    vupdate_lane_int32(C318, A3, B4, 2); 
    vupdate_lane_int32(C319, A3, B4, 3); 
    bA+=16;
    bB+=20;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(0,12), C012); 
  vstoreC_int32(&Crref(0,13), C013); 
  vstoreC_int32(&Crref(0,14), C014); 
  vstoreC_int32(&Crref(0,15), C015); 
  vstoreC_int32(&Crref(0,16), C016); 
  vstoreC_int32(&Crref(0,17), C017); 
  vstoreC_int32(&Crref(0,18), C018); 
  vstoreC_int32(&Crref(0,19), C019); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(4,8), C18); 
  vstoreC_int32(&Crref(4,9), C19); 
  vstoreC_int32(&Crref(4,10), C110); 
  vstoreC_int32(&Crref(4,11), C111); 
  vstoreC_int32(&Crref(4,12), C112); 
  vstoreC_int32(&Crref(4,13), C113); 
  vstoreC_int32(&Crref(4,14), C114); 
  vstoreC_int32(&Crref(4,15), C115); 
  vstoreC_int32(&Crref(4,16), C116); 
  vstoreC_int32(&Crref(4,17), C117); 
  vstoreC_int32(&Crref(4,18), C118); 
  vstoreC_int32(&Crref(4,19), C119); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(8,4), C24); 
  vstoreC_int32(&Crref(8,5), C25); 
  vstoreC_int32(&Crref(8,6), C26); 
  vstoreC_int32(&Crref(8,7), C27); 
  vstoreC_int32(&Crref(8,8), C28); 
  vstoreC_int32(&Crref(8,9), C29); 
  vstoreC_int32(&Crref(8,10), C210); 
  vstoreC_int32(&Crref(8,11), C211); 
  vstoreC_int32(&Crref(8,12), C212); 
  vstoreC_int32(&Crref(8,13), C213); 
  vstoreC_int32(&Crref(8,14), C214); 
  vstoreC_int32(&Crref(8,15), C215); 
  vstoreC_int32(&Crref(8,16), C216); 
  vstoreC_int32(&Crref(8,17), C217); 
  vstoreC_int32(&Crref(8,18), C218); 
  vstoreC_int32(&Crref(8,19), C219); 
  vstoreC_int32(&Crref(12,0), C30); 
  vstoreC_int32(&Crref(12,1), C31); 
  vstoreC_int32(&Crref(12,2), C32); 
  vstoreC_int32(&Crref(12,3), C33); 
  vstoreC_int32(&Crref(12,4), C34); 
  vstoreC_int32(&Crref(12,5), C35); 
  vstoreC_int32(&Crref(12,6), C36); 
  vstoreC_int32(&Crref(12,7), C37); 
  vstoreC_int32(&Crref(12,8), C38); 
  vstoreC_int32(&Crref(12,9), C39); 
  vstoreC_int32(&Crref(12,10), C310); 
  vstoreC_int32(&Crref(12,11), C311); 
  vstoreC_int32(&Crref(12,12), C312); 
  vstoreC_int32(&Crref(12,13), C313); 
  vstoreC_int32(&Crref(12,14), C314); 
  vstoreC_int32(&Crref(12,15), C315); 
  vstoreC_int32(&Crref(12,16), C316); 
  vstoreC_int32(&Crref(12,17), C317); 
  vstoreC_int32(&Crref(12,18), C318); 
  vstoreC_int32(&Crref(12,19), C319); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_20x4_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=20;
  const int NR=4;
  int32_t Ctmp[20*4];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  A3,  A4,  B0;
  int32x4_t  C00,  C01,  C02,  C03,  C10,  C11,  C12,  C13,  C20,  C21,  C22,  C23,  C30,  C31,  C32,  C33,  C40,  C41,  C42,  C43;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C30, 0);
    vinit_int32(C31, 0);
    vinit_int32(C32, 0);
    vinit_int32(C33, 0);
    vinit_int32(C40, 0);
    vinit_int32(C41, 0);
    vinit_int32(C42, 0);
    vinit_int32(C43, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C30, &Crref(12, 0));
    vloadC_int32(C31, &Crref(12, 1));
    vloadC_int32(C32, &Crref(12, 2));
    vloadC_int32(C33, &Crref(12, 3));
    vloadC_int32(C40, &Crref(16, 0));
    vloadC_int32(C41, &Crref(16, 1));
    vloadC_int32(C42, &Crref(16, 2));
    vloadC_int32(C43, &Crref(16, 3));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(A3, &Ar[bA + 12]);
    vload_int32(A4, &Ar[bA + 16]);
    vload_int32(B0, &Br[bB + 0]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C30, A3, B0, 0); 
    vupdate_lane_int32(C31, A3, B0, 1); 
    vupdate_lane_int32(C32, A3, B0, 2); 
    vupdate_lane_int32(C33, A3, B0, 3); 
    vupdate_lane_int32(C40, A4, B0, 0); 
    vupdate_lane_int32(C41, A4, B0, 1); 
    vupdate_lane_int32(C42, A4, B0, 2); 
    vupdate_lane_int32(C43, A4, B0, 3); 
    bA+=20;
    bB+=4;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(12,0), C30); 
  vstoreC_int32(&Crref(12,1), C31); 
  vstoreC_int32(&Crref(12,2), C32); 
  vstoreC_int32(&Crref(12,3), C33); 
  vstoreC_int32(&Crref(16,0), C40); 
  vstoreC_int32(&Crref(16,1), C41); 
  vstoreC_int32(&Crref(16,2), C42); 
  vstoreC_int32(&Crref(16,3), C43); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_20x8_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=20;
  const int NR=8;
  int32_t Ctmp[20*8];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  A3,  A4,  B0,  B1;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C20,  C21,  C22,  C23,  C24,  C25,  C26,  C27,  C30,  C31,  C32,  C33,  C34,  C35,  C36,  C37,  C40,  C41,  C42,  C43,  C44,  C45,  C46,  C47;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C24, 0);
    vinit_int32(C25, 0);
    vinit_int32(C26, 0);
    vinit_int32(C27, 0);
    vinit_int32(C30, 0);
    vinit_int32(C31, 0);
    vinit_int32(C32, 0);
    vinit_int32(C33, 0);
    vinit_int32(C34, 0);
    vinit_int32(C35, 0);
    vinit_int32(C36, 0);
    vinit_int32(C37, 0);
    vinit_int32(C40, 0);
    vinit_int32(C41, 0);
    vinit_int32(C42, 0);
    vinit_int32(C43, 0);
    vinit_int32(C44, 0);
    vinit_int32(C45, 0);
    vinit_int32(C46, 0);
    vinit_int32(C47, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C24, &Crref(8, 4));
    vloadC_int32(C25, &Crref(8, 5));
    vloadC_int32(C26, &Crref(8, 6));
    vloadC_int32(C27, &Crref(8, 7));
    vloadC_int32(C30, &Crref(12, 0));
    vloadC_int32(C31, &Crref(12, 1));
    vloadC_int32(C32, &Crref(12, 2));
    vloadC_int32(C33, &Crref(12, 3));
    vloadC_int32(C34, &Crref(12, 4));
    vloadC_int32(C35, &Crref(12, 5));
    vloadC_int32(C36, &Crref(12, 6));
    vloadC_int32(C37, &Crref(12, 7));
    vloadC_int32(C40, &Crref(16, 0));
    vloadC_int32(C41, &Crref(16, 1));
    vloadC_int32(C42, &Crref(16, 2));
    vloadC_int32(C43, &Crref(16, 3));
    vloadC_int32(C44, &Crref(16, 4));
    vloadC_int32(C45, &Crref(16, 5));
    vloadC_int32(C46, &Crref(16, 6));
    vloadC_int32(C47, &Crref(16, 7));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(A3, &Ar[bA + 12]);
    vload_int32(A4, &Ar[bA + 16]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C24, A2, B1, 0); 
    vupdate_lane_int32(C25, A2, B1, 1); 
    vupdate_lane_int32(C26, A2, B1, 2); 
    vupdate_lane_int32(C27, A2, B1, 3); 
    vupdate_lane_int32(C30, A3, B0, 0); 
    vupdate_lane_int32(C31, A3, B0, 1); 
    vupdate_lane_int32(C32, A3, B0, 2); 
    vupdate_lane_int32(C33, A3, B0, 3); 
    vupdate_lane_int32(C34, A3, B1, 0); 
    vupdate_lane_int32(C35, A3, B1, 1); 
    vupdate_lane_int32(C36, A3, B1, 2); 
    vupdate_lane_int32(C37, A3, B1, 3); 
    vupdate_lane_int32(C40, A4, B0, 0); 
    vupdate_lane_int32(C41, A4, B0, 1); 
    vupdate_lane_int32(C42, A4, B0, 2); 
    vupdate_lane_int32(C43, A4, B0, 3); 
    vupdate_lane_int32(C44, A4, B1, 0); 
    vupdate_lane_int32(C45, A4, B1, 1); 
    vupdate_lane_int32(C46, A4, B1, 2); 
    vupdate_lane_int32(C47, A4, B1, 3); 
    bA+=20;
    bB+=8;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(8,4), C24); 
  vstoreC_int32(&Crref(8,5), C25); 
  vstoreC_int32(&Crref(8,6), C26); 
  vstoreC_int32(&Crref(8,7), C27); 
  vstoreC_int32(&Crref(12,0), C30); 
  vstoreC_int32(&Crref(12,1), C31); 
  vstoreC_int32(&Crref(12,2), C32); 
  vstoreC_int32(&Crref(12,3), C33); 
  vstoreC_int32(&Crref(12,4), C34); 
  vstoreC_int32(&Crref(12,5), C35); 
  vstoreC_int32(&Crref(12,6), C36); 
  vstoreC_int32(&Crref(12,7), C37); 
  vstoreC_int32(&Crref(16,0), C40); 
  vstoreC_int32(&Crref(16,1), C41); 
  vstoreC_int32(&Crref(16,2), C42); 
  vstoreC_int32(&Crref(16,3), C43); 
  vstoreC_int32(&Crref(16,4), C44); 
  vstoreC_int32(&Crref(16,5), C45); 
  vstoreC_int32(&Crref(16,6), C46); 
  vstoreC_int32(&Crref(16,7), C47); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_20x12_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=20;
  const int NR=12;
  int32_t Ctmp[20*12];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  A3,  A4,  B0,  B1,  B2;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C18,  C19,  C110,  C111,  C20,  C21,  C22,  C23,  C24,  C25,  C26,  C27,  C28,  C29,  C210,  C211,  C30,  C31,  C32,  C33,  C34,  C35,  C36,  C37,  C38,  C39,  C310,  C311,  C40,  C41,  C42,  C43,  C44,  C45,  C46,  C47,  C48,  C49,  C410,  C411;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C18, 0);
    vinit_int32(C19, 0);
    vinit_int32(C110, 0);
    vinit_int32(C111, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C24, 0);
    vinit_int32(C25, 0);
    vinit_int32(C26, 0);
    vinit_int32(C27, 0);
    vinit_int32(C28, 0);
    vinit_int32(C29, 0);
    vinit_int32(C210, 0);
    vinit_int32(C211, 0);
    vinit_int32(C30, 0);
    vinit_int32(C31, 0);
    vinit_int32(C32, 0);
    vinit_int32(C33, 0);
    vinit_int32(C34, 0);
    vinit_int32(C35, 0);
    vinit_int32(C36, 0);
    vinit_int32(C37, 0);
    vinit_int32(C38, 0);
    vinit_int32(C39, 0);
    vinit_int32(C310, 0);
    vinit_int32(C311, 0);
    vinit_int32(C40, 0);
    vinit_int32(C41, 0);
    vinit_int32(C42, 0);
    vinit_int32(C43, 0);
    vinit_int32(C44, 0);
    vinit_int32(C45, 0);
    vinit_int32(C46, 0);
    vinit_int32(C47, 0);
    vinit_int32(C48, 0);
    vinit_int32(C49, 0);
    vinit_int32(C410, 0);
    vinit_int32(C411, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C18, &Crref(4, 8));
    vloadC_int32(C19, &Crref(4, 9));
    vloadC_int32(C110, &Crref(4, 10));
    vloadC_int32(C111, &Crref(4, 11));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C24, &Crref(8, 4));
    vloadC_int32(C25, &Crref(8, 5));
    vloadC_int32(C26, &Crref(8, 6));
    vloadC_int32(C27, &Crref(8, 7));
    vloadC_int32(C28, &Crref(8, 8));
    vloadC_int32(C29, &Crref(8, 9));
    vloadC_int32(C210, &Crref(8, 10));
    vloadC_int32(C211, &Crref(8, 11));
    vloadC_int32(C30, &Crref(12, 0));
    vloadC_int32(C31, &Crref(12, 1));
    vloadC_int32(C32, &Crref(12, 2));
    vloadC_int32(C33, &Crref(12, 3));
    vloadC_int32(C34, &Crref(12, 4));
    vloadC_int32(C35, &Crref(12, 5));
    vloadC_int32(C36, &Crref(12, 6));
    vloadC_int32(C37, &Crref(12, 7));
    vloadC_int32(C38, &Crref(12, 8));
    vloadC_int32(C39, &Crref(12, 9));
    vloadC_int32(C310, &Crref(12, 10));
    vloadC_int32(C311, &Crref(12, 11));
    vloadC_int32(C40, &Crref(16, 0));
    vloadC_int32(C41, &Crref(16, 1));
    vloadC_int32(C42, &Crref(16, 2));
    vloadC_int32(C43, &Crref(16, 3));
    vloadC_int32(C44, &Crref(16, 4));
    vloadC_int32(C45, &Crref(16, 5));
    vloadC_int32(C46, &Crref(16, 6));
    vloadC_int32(C47, &Crref(16, 7));
    vloadC_int32(C48, &Crref(16, 8));
    vloadC_int32(C49, &Crref(16, 9));
    vloadC_int32(C410, &Crref(16, 10));
    vloadC_int32(C411, &Crref(16, 11));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(A3, &Ar[bA + 12]);
    vload_int32(A4, &Ar[bA + 16]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C18, A1, B2, 0); 
    vupdate_lane_int32(C19, A1, B2, 1); 
    vupdate_lane_int32(C110, A1, B2, 2); 
    vupdate_lane_int32(C111, A1, B2, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C24, A2, B1, 0); 
    vupdate_lane_int32(C25, A2, B1, 1); 
    vupdate_lane_int32(C26, A2, B1, 2); 
    vupdate_lane_int32(C27, A2, B1, 3); 
    vupdate_lane_int32(C28, A2, B2, 0); 
    vupdate_lane_int32(C29, A2, B2, 1); 
    vupdate_lane_int32(C210, A2, B2, 2); 
    vupdate_lane_int32(C211, A2, B2, 3); 
    vupdate_lane_int32(C30, A3, B0, 0); 
    vupdate_lane_int32(C31, A3, B0, 1); 
    vupdate_lane_int32(C32, A3, B0, 2); 
    vupdate_lane_int32(C33, A3, B0, 3); 
    vupdate_lane_int32(C34, A3, B1, 0); 
    vupdate_lane_int32(C35, A3, B1, 1); 
    vupdate_lane_int32(C36, A3, B1, 2); 
    vupdate_lane_int32(C37, A3, B1, 3); 
    vupdate_lane_int32(C38, A3, B2, 0); 
    vupdate_lane_int32(C39, A3, B2, 1); 
    vupdate_lane_int32(C310, A3, B2, 2); 
    vupdate_lane_int32(C311, A3, B2, 3); 
    vupdate_lane_int32(C40, A4, B0, 0); 
    vupdate_lane_int32(C41, A4, B0, 1); 
    vupdate_lane_int32(C42, A4, B0, 2); 
    vupdate_lane_int32(C43, A4, B0, 3); 
    vupdate_lane_int32(C44, A4, B1, 0); 
    vupdate_lane_int32(C45, A4, B1, 1); 
    vupdate_lane_int32(C46, A4, B1, 2); 
    vupdate_lane_int32(C47, A4, B1, 3); 
    vupdate_lane_int32(C48, A4, B2, 0); 
    vupdate_lane_int32(C49, A4, B2, 1); 
    vupdate_lane_int32(C410, A4, B2, 2); 
    vupdate_lane_int32(C411, A4, B2, 3); 
    bA+=20;
    bB+=12;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(4,8), C18); 
  vstoreC_int32(&Crref(4,9), C19); 
  vstoreC_int32(&Crref(4,10), C110); 
  vstoreC_int32(&Crref(4,11), C111); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(8,4), C24); 
  vstoreC_int32(&Crref(8,5), C25); 
  vstoreC_int32(&Crref(8,6), C26); 
  vstoreC_int32(&Crref(8,7), C27); 
  vstoreC_int32(&Crref(8,8), C28); 
  vstoreC_int32(&Crref(8,9), C29); 
  vstoreC_int32(&Crref(8,10), C210); 
  vstoreC_int32(&Crref(8,11), C211); 
  vstoreC_int32(&Crref(12,0), C30); 
  vstoreC_int32(&Crref(12,1), C31); 
  vstoreC_int32(&Crref(12,2), C32); 
  vstoreC_int32(&Crref(12,3), C33); 
  vstoreC_int32(&Crref(12,4), C34); 
  vstoreC_int32(&Crref(12,5), C35); 
  vstoreC_int32(&Crref(12,6), C36); 
  vstoreC_int32(&Crref(12,7), C37); 
  vstoreC_int32(&Crref(12,8), C38); 
  vstoreC_int32(&Crref(12,9), C39); 
  vstoreC_int32(&Crref(12,10), C310); 
  vstoreC_int32(&Crref(12,11), C311); 
  vstoreC_int32(&Crref(16,0), C40); 
  vstoreC_int32(&Crref(16,1), C41); 
  vstoreC_int32(&Crref(16,2), C42); 
  vstoreC_int32(&Crref(16,3), C43); 
  vstoreC_int32(&Crref(16,4), C44); 
  vstoreC_int32(&Crref(16,5), C45); 
  vstoreC_int32(&Crref(16,6), C46); 
  vstoreC_int32(&Crref(16,7), C47); 
  vstoreC_int32(&Crref(16,8), C48); 
  vstoreC_int32(&Crref(16,9), C49); 
  vstoreC_int32(&Crref(16,10), C410); 
  vstoreC_int32(&Crref(16,11), C411); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_20x16_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=20;
  const int NR=16;
  int32_t Ctmp[20*16];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  A3,  A4,  B0,  B1,  B2,  B3;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C012,  C013,  C014,  C015,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C18,  C19,  C110,  C111,  C112,  C113,  C114,  C115,  C20,  C21,  C22,  C23,  C24,  C25,  C26,  C27,  C28,  C29,  C210,  C211,  C212,  C213,  C214,  C215,  C30,  C31,  C32,  C33,  C34,  C35,  C36,  C37,  C38,  C39,  C310,  C311,  C312,  C313,  C314,  C315,  C40,  C41,  C42,  C43,  C44,  C45,  C46,  C47,  C48,  C49,  C410,  C411,  C412,  C413,  C414,  C415;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C012, 0);
    vinit_int32(C013, 0);
    vinit_int32(C014, 0);
    vinit_int32(C015, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C18, 0);
    vinit_int32(C19, 0);
    vinit_int32(C110, 0);
    vinit_int32(C111, 0);
    vinit_int32(C112, 0);
    vinit_int32(C113, 0);
    vinit_int32(C114, 0);
    vinit_int32(C115, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C24, 0);
    vinit_int32(C25, 0);
    vinit_int32(C26, 0);
    vinit_int32(C27, 0);
    vinit_int32(C28, 0);
    vinit_int32(C29, 0);
    vinit_int32(C210, 0);
    vinit_int32(C211, 0);
    vinit_int32(C212, 0);
    vinit_int32(C213, 0);
    vinit_int32(C214, 0);
    vinit_int32(C215, 0);
    vinit_int32(C30, 0);
    vinit_int32(C31, 0);
    vinit_int32(C32, 0);
    vinit_int32(C33, 0);
    vinit_int32(C34, 0);
    vinit_int32(C35, 0);
    vinit_int32(C36, 0);
    vinit_int32(C37, 0);
    vinit_int32(C38, 0);
    vinit_int32(C39, 0);
    vinit_int32(C310, 0);
    vinit_int32(C311, 0);
    vinit_int32(C312, 0);
    vinit_int32(C313, 0);
    vinit_int32(C314, 0);
    vinit_int32(C315, 0);
    vinit_int32(C40, 0);
    vinit_int32(C41, 0);
    vinit_int32(C42, 0);
    vinit_int32(C43, 0);
    vinit_int32(C44, 0);
    vinit_int32(C45, 0);
    vinit_int32(C46, 0);
    vinit_int32(C47, 0);
    vinit_int32(C48, 0);
    vinit_int32(C49, 0);
    vinit_int32(C410, 0);
    vinit_int32(C411, 0);
    vinit_int32(C412, 0);
    vinit_int32(C413, 0);
    vinit_int32(C414, 0);
    vinit_int32(C415, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C012, &Crref(0, 12));
    vloadC_int32(C013, &Crref(0, 13));
    vloadC_int32(C014, &Crref(0, 14));
    vloadC_int32(C015, &Crref(0, 15));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C18, &Crref(4, 8));
    vloadC_int32(C19, &Crref(4, 9));
    vloadC_int32(C110, &Crref(4, 10));
    vloadC_int32(C111, &Crref(4, 11));
    vloadC_int32(C112, &Crref(4, 12));
    vloadC_int32(C113, &Crref(4, 13));
    vloadC_int32(C114, &Crref(4, 14));
    vloadC_int32(C115, &Crref(4, 15));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C24, &Crref(8, 4));
    vloadC_int32(C25, &Crref(8, 5));
    vloadC_int32(C26, &Crref(8, 6));
    vloadC_int32(C27, &Crref(8, 7));
    vloadC_int32(C28, &Crref(8, 8));
    vloadC_int32(C29, &Crref(8, 9));
    vloadC_int32(C210, &Crref(8, 10));
    vloadC_int32(C211, &Crref(8, 11));
    vloadC_int32(C212, &Crref(8, 12));
    vloadC_int32(C213, &Crref(8, 13));
    vloadC_int32(C214, &Crref(8, 14));
    vloadC_int32(C215, &Crref(8, 15));
    vloadC_int32(C30, &Crref(12, 0));
    vloadC_int32(C31, &Crref(12, 1));
    vloadC_int32(C32, &Crref(12, 2));
    vloadC_int32(C33, &Crref(12, 3));
    vloadC_int32(C34, &Crref(12, 4));
    vloadC_int32(C35, &Crref(12, 5));
    vloadC_int32(C36, &Crref(12, 6));
    vloadC_int32(C37, &Crref(12, 7));
    vloadC_int32(C38, &Crref(12, 8));
    vloadC_int32(C39, &Crref(12, 9));
    vloadC_int32(C310, &Crref(12, 10));
    vloadC_int32(C311, &Crref(12, 11));
    vloadC_int32(C312, &Crref(12, 12));
    vloadC_int32(C313, &Crref(12, 13));
    vloadC_int32(C314, &Crref(12, 14));
    vloadC_int32(C315, &Crref(12, 15));
    vloadC_int32(C40, &Crref(16, 0));
    vloadC_int32(C41, &Crref(16, 1));
    vloadC_int32(C42, &Crref(16, 2));
    vloadC_int32(C43, &Crref(16, 3));
    vloadC_int32(C44, &Crref(16, 4));
    vloadC_int32(C45, &Crref(16, 5));
    vloadC_int32(C46, &Crref(16, 6));
    vloadC_int32(C47, &Crref(16, 7));
    vloadC_int32(C48, &Crref(16, 8));
    vloadC_int32(C49, &Crref(16, 9));
    vloadC_int32(C410, &Crref(16, 10));
    vloadC_int32(C411, &Crref(16, 11));
    vloadC_int32(C412, &Crref(16, 12));
    vloadC_int32(C413, &Crref(16, 13));
    vloadC_int32(C414, &Crref(16, 14));
    vloadC_int32(C415, &Crref(16, 15));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(A3, &Ar[bA + 12]);
    vload_int32(A4, &Ar[bA + 16]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vload_int32(B3, &Br[bB + 12]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C012, A0, B3, 0); 
    vupdate_lane_int32(C013, A0, B3, 1); 
    vupdate_lane_int32(C014, A0, B3, 2); 
    vupdate_lane_int32(C015, A0, B3, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C18, A1, B2, 0); 
    vupdate_lane_int32(C19, A1, B2, 1); 
    vupdate_lane_int32(C110, A1, B2, 2); 
    vupdate_lane_int32(C111, A1, B2, 3); 
    vupdate_lane_int32(C112, A1, B3, 0); 
    vupdate_lane_int32(C113, A1, B3, 1); 
    vupdate_lane_int32(C114, A1, B3, 2); 
    vupdate_lane_int32(C115, A1, B3, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C24, A2, B1, 0); 
    vupdate_lane_int32(C25, A2, B1, 1); 
    vupdate_lane_int32(C26, A2, B1, 2); 
    vupdate_lane_int32(C27, A2, B1, 3); 
    vupdate_lane_int32(C28, A2, B2, 0); 
    vupdate_lane_int32(C29, A2, B2, 1); 
    vupdate_lane_int32(C210, A2, B2, 2); 
    vupdate_lane_int32(C211, A2, B2, 3); 
    vupdate_lane_int32(C212, A2, B3, 0); 
    vupdate_lane_int32(C213, A2, B3, 1); 
    vupdate_lane_int32(C214, A2, B3, 2); 
    vupdate_lane_int32(C215, A2, B3, 3); 
    vupdate_lane_int32(C30, A3, B0, 0); 
    vupdate_lane_int32(C31, A3, B0, 1); 
    vupdate_lane_int32(C32, A3, B0, 2); 
    vupdate_lane_int32(C33, A3, B0, 3); 
    vupdate_lane_int32(C34, A3, B1, 0); 
    vupdate_lane_int32(C35, A3, B1, 1); 
    vupdate_lane_int32(C36, A3, B1, 2); 
    vupdate_lane_int32(C37, A3, B1, 3); 
    vupdate_lane_int32(C38, A3, B2, 0); 
    vupdate_lane_int32(C39, A3, B2, 1); 
    vupdate_lane_int32(C310, A3, B2, 2); 
    vupdate_lane_int32(C311, A3, B2, 3); 
    vupdate_lane_int32(C312, A3, B3, 0); 
    vupdate_lane_int32(C313, A3, B3, 1); 
    vupdate_lane_int32(C314, A3, B3, 2); 
    vupdate_lane_int32(C315, A3, B3, 3); 
    vupdate_lane_int32(C40, A4, B0, 0); 
    vupdate_lane_int32(C41, A4, B0, 1); 
    vupdate_lane_int32(C42, A4, B0, 2); 
    vupdate_lane_int32(C43, A4, B0, 3); 
    vupdate_lane_int32(C44, A4, B1, 0); 
    vupdate_lane_int32(C45, A4, B1, 1); 
    vupdate_lane_int32(C46, A4, B1, 2); 
    vupdate_lane_int32(C47, A4, B1, 3); 
    vupdate_lane_int32(C48, A4, B2, 0); 
    vupdate_lane_int32(C49, A4, B2, 1); 
    vupdate_lane_int32(C410, A4, B2, 2); 
    vupdate_lane_int32(C411, A4, B2, 3); 
    vupdate_lane_int32(C412, A4, B3, 0); 
    vupdate_lane_int32(C413, A4, B3, 1); 
    vupdate_lane_int32(C414, A4, B3, 2); 
    vupdate_lane_int32(C415, A4, B3, 3); 
    bA+=20;
    bB+=16;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(0,12), C012); 
  vstoreC_int32(&Crref(0,13), C013); 
  vstoreC_int32(&Crref(0,14), C014); 
  vstoreC_int32(&Crref(0,15), C015); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(4,8), C18); 
  vstoreC_int32(&Crref(4,9), C19); 
  vstoreC_int32(&Crref(4,10), C110); 
  vstoreC_int32(&Crref(4,11), C111); 
  vstoreC_int32(&Crref(4,12), C112); 
  vstoreC_int32(&Crref(4,13), C113); 
  vstoreC_int32(&Crref(4,14), C114); 
  vstoreC_int32(&Crref(4,15), C115); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(8,4), C24); 
  vstoreC_int32(&Crref(8,5), C25); 
  vstoreC_int32(&Crref(8,6), C26); 
  vstoreC_int32(&Crref(8,7), C27); 
  vstoreC_int32(&Crref(8,8), C28); 
  vstoreC_int32(&Crref(8,9), C29); 
  vstoreC_int32(&Crref(8,10), C210); 
  vstoreC_int32(&Crref(8,11), C211); 
  vstoreC_int32(&Crref(8,12), C212); 
  vstoreC_int32(&Crref(8,13), C213); 
  vstoreC_int32(&Crref(8,14), C214); 
  vstoreC_int32(&Crref(8,15), C215); 
  vstoreC_int32(&Crref(12,0), C30); 
  vstoreC_int32(&Crref(12,1), C31); 
  vstoreC_int32(&Crref(12,2), C32); 
  vstoreC_int32(&Crref(12,3), C33); 
  vstoreC_int32(&Crref(12,4), C34); 
  vstoreC_int32(&Crref(12,5), C35); 
  vstoreC_int32(&Crref(12,6), C36); 
  vstoreC_int32(&Crref(12,7), C37); 
  vstoreC_int32(&Crref(12,8), C38); 
  vstoreC_int32(&Crref(12,9), C39); 
  vstoreC_int32(&Crref(12,10), C310); 
  vstoreC_int32(&Crref(12,11), C311); 
  vstoreC_int32(&Crref(12,12), C312); 
  vstoreC_int32(&Crref(12,13), C313); 
  vstoreC_int32(&Crref(12,14), C314); 
  vstoreC_int32(&Crref(12,15), C315); 
  vstoreC_int32(&Crref(16,0), C40); 
  vstoreC_int32(&Crref(16,1), C41); 
  vstoreC_int32(&Crref(16,2), C42); 
  vstoreC_int32(&Crref(16,3), C43); 
  vstoreC_int32(&Crref(16,4), C44); 
  vstoreC_int32(&Crref(16,5), C45); 
  vstoreC_int32(&Crref(16,6), C46); 
  vstoreC_int32(&Crref(16,7), C47); 
  vstoreC_int32(&Crref(16,8), C48); 
  vstoreC_int32(&Crref(16,9), C49); 
  vstoreC_int32(&Crref(16,10), C410); 
  vstoreC_int32(&Crref(16,11), C411); 
  vstoreC_int32(&Crref(16,12), C412); 
  vstoreC_int32(&Crref(16,13), C413); 
  vstoreC_int32(&Crref(16,14), C414); 
  vstoreC_int32(&Crref(16,15), C415); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}
void ukernel_intrinsic_20x20_int32(int mr, int nr, int kc, int32_t  *Ar, int32_t *Br, int32_t *Cor, int32_t betaI, int Clda){
  int pr, bA = 0, bB = 0;
  int ldC;
  const int MR=20;
  const int NR=20;
  int32_t Ctmp[20*20];
  int32_t beta;
  int32_t *Cr;
  if (mr == MR && nr == NR) {Cr=Cor; ldC = Clda; beta=betaI;} else {Cr=Ctmp; ldC = MR; beta=0;}
  int32x4_t  vtmp;
  int32x4_t  A0,  A1,  A2,  A3,  A4,  B0,  B1,  B2,  B3,  B4;
  int32x4_t  C00,  C01,  C02,  C03,  C04,  C05,  C06,  C07,  C08,  C09,  C010,  C011,  C012,  C013,  C014,  C015,  C016,  C017,  C018,  C019,  C10,  C11,  C12,  C13,  C14,  C15,  C16,  C17,  C18,  C19,  C110,  C111,  C112,  C113,  C114,  C115,  C116,  C117,  C118,  C119,  C20,  C21,  C22,  C23,  C24,  C25,  C26,  C27,  C28,  C29,  C210,  C211,  C212,  C213,  C214,  C215,  C216,  C217,  C218,  C219,  C30,  C31,  C32,  C33,  C34,  C35,  C36,  C37,  C38,  C39,  C310,  C311,  C312,  C313,  C314,  C315,  C316,  C317,  C318,  C319,  C40,  C41,  C42,  C43,  C44,  C45,  C46,  C47,  C48,  C49,  C410,  C411,  C412,  C413,  C414,  C415,  C416,  C417,  C418,  C419;

  if (beta == 0) {
    vinit_int32(C00, 0);
    vinit_int32(C01, 0);
    vinit_int32(C02, 0);
    vinit_int32(C03, 0);
    vinit_int32(C04, 0);
    vinit_int32(C05, 0);
    vinit_int32(C06, 0);
    vinit_int32(C07, 0);
    vinit_int32(C08, 0);
    vinit_int32(C09, 0);
    vinit_int32(C010, 0);
    vinit_int32(C011, 0);
    vinit_int32(C012, 0);
    vinit_int32(C013, 0);
    vinit_int32(C014, 0);
    vinit_int32(C015, 0);
    vinit_int32(C016, 0);
    vinit_int32(C017, 0);
    vinit_int32(C018, 0);
    vinit_int32(C019, 0);
    vinit_int32(C10, 0);
    vinit_int32(C11, 0);
    vinit_int32(C12, 0);
    vinit_int32(C13, 0);
    vinit_int32(C14, 0);
    vinit_int32(C15, 0);
    vinit_int32(C16, 0);
    vinit_int32(C17, 0);
    vinit_int32(C18, 0);
    vinit_int32(C19, 0);
    vinit_int32(C110, 0);
    vinit_int32(C111, 0);
    vinit_int32(C112, 0);
    vinit_int32(C113, 0);
    vinit_int32(C114, 0);
    vinit_int32(C115, 0);
    vinit_int32(C116, 0);
    vinit_int32(C117, 0);
    vinit_int32(C118, 0);
    vinit_int32(C119, 0);
    vinit_int32(C20, 0);
    vinit_int32(C21, 0);
    vinit_int32(C22, 0);
    vinit_int32(C23, 0);
    vinit_int32(C24, 0);
    vinit_int32(C25, 0);
    vinit_int32(C26, 0);
    vinit_int32(C27, 0);
    vinit_int32(C28, 0);
    vinit_int32(C29, 0);
    vinit_int32(C210, 0);
    vinit_int32(C211, 0);
    vinit_int32(C212, 0);
    vinit_int32(C213, 0);
    vinit_int32(C214, 0);
    vinit_int32(C215, 0);
    vinit_int32(C216, 0);
    vinit_int32(C217, 0);
    vinit_int32(C218, 0);
    vinit_int32(C219, 0);
    vinit_int32(C30, 0);
    vinit_int32(C31, 0);
    vinit_int32(C32, 0);
    vinit_int32(C33, 0);
    vinit_int32(C34, 0);
    vinit_int32(C35, 0);
    vinit_int32(C36, 0);
    vinit_int32(C37, 0);
    vinit_int32(C38, 0);
    vinit_int32(C39, 0);
    vinit_int32(C310, 0);
    vinit_int32(C311, 0);
    vinit_int32(C312, 0);
    vinit_int32(C313, 0);
    vinit_int32(C314, 0);
    vinit_int32(C315, 0);
    vinit_int32(C316, 0);
    vinit_int32(C317, 0);
    vinit_int32(C318, 0);
    vinit_int32(C319, 0);
    vinit_int32(C40, 0);
    vinit_int32(C41, 0);
    vinit_int32(C42, 0);
    vinit_int32(C43, 0);
    vinit_int32(C44, 0);
    vinit_int32(C45, 0);
    vinit_int32(C46, 0);
    vinit_int32(C47, 0);
    vinit_int32(C48, 0);
    vinit_int32(C49, 0);
    vinit_int32(C410, 0);
    vinit_int32(C411, 0);
    vinit_int32(C412, 0);
    vinit_int32(C413, 0);
    vinit_int32(C414, 0);
    vinit_int32(C415, 0);
    vinit_int32(C416, 0);
    vinit_int32(C417, 0);
    vinit_int32(C418, 0);
    vinit_int32(C419, 0);
  } else {
    vloadC_int32(C00, &Crref(0, 0));
    vloadC_int32(C01, &Crref(0, 1));
    vloadC_int32(C02, &Crref(0, 2));
    vloadC_int32(C03, &Crref(0, 3));
    vloadC_int32(C04, &Crref(0, 4));
    vloadC_int32(C05, &Crref(0, 5));
    vloadC_int32(C06, &Crref(0, 6));
    vloadC_int32(C07, &Crref(0, 7));
    vloadC_int32(C08, &Crref(0, 8));
    vloadC_int32(C09, &Crref(0, 9));
    vloadC_int32(C010, &Crref(0, 10));
    vloadC_int32(C011, &Crref(0, 11));
    vloadC_int32(C012, &Crref(0, 12));
    vloadC_int32(C013, &Crref(0, 13));
    vloadC_int32(C014, &Crref(0, 14));
    vloadC_int32(C015, &Crref(0, 15));
    vloadC_int32(C016, &Crref(0, 16));
    vloadC_int32(C017, &Crref(0, 17));
    vloadC_int32(C018, &Crref(0, 18));
    vloadC_int32(C019, &Crref(0, 19));
    vloadC_int32(C10, &Crref(4, 0));
    vloadC_int32(C11, &Crref(4, 1));
    vloadC_int32(C12, &Crref(4, 2));
    vloadC_int32(C13, &Crref(4, 3));
    vloadC_int32(C14, &Crref(4, 4));
    vloadC_int32(C15, &Crref(4, 5));
    vloadC_int32(C16, &Crref(4, 6));
    vloadC_int32(C17, &Crref(4, 7));
    vloadC_int32(C18, &Crref(4, 8));
    vloadC_int32(C19, &Crref(4, 9));
    vloadC_int32(C110, &Crref(4, 10));
    vloadC_int32(C111, &Crref(4, 11));
    vloadC_int32(C112, &Crref(4, 12));
    vloadC_int32(C113, &Crref(4, 13));
    vloadC_int32(C114, &Crref(4, 14));
    vloadC_int32(C115, &Crref(4, 15));
    vloadC_int32(C116, &Crref(4, 16));
    vloadC_int32(C117, &Crref(4, 17));
    vloadC_int32(C118, &Crref(4, 18));
    vloadC_int32(C119, &Crref(4, 19));
    vloadC_int32(C20, &Crref(8, 0));
    vloadC_int32(C21, &Crref(8, 1));
    vloadC_int32(C22, &Crref(8, 2));
    vloadC_int32(C23, &Crref(8, 3));
    vloadC_int32(C24, &Crref(8, 4));
    vloadC_int32(C25, &Crref(8, 5));
    vloadC_int32(C26, &Crref(8, 6));
    vloadC_int32(C27, &Crref(8, 7));
    vloadC_int32(C28, &Crref(8, 8));
    vloadC_int32(C29, &Crref(8, 9));
    vloadC_int32(C210, &Crref(8, 10));
    vloadC_int32(C211, &Crref(8, 11));
    vloadC_int32(C212, &Crref(8, 12));
    vloadC_int32(C213, &Crref(8, 13));
    vloadC_int32(C214, &Crref(8, 14));
    vloadC_int32(C215, &Crref(8, 15));
    vloadC_int32(C216, &Crref(8, 16));
    vloadC_int32(C217, &Crref(8, 17));
    vloadC_int32(C218, &Crref(8, 18));
    vloadC_int32(C219, &Crref(8, 19));
    vloadC_int32(C30, &Crref(12, 0));
    vloadC_int32(C31, &Crref(12, 1));
    vloadC_int32(C32, &Crref(12, 2));
    vloadC_int32(C33, &Crref(12, 3));
    vloadC_int32(C34, &Crref(12, 4));
    vloadC_int32(C35, &Crref(12, 5));
    vloadC_int32(C36, &Crref(12, 6));
    vloadC_int32(C37, &Crref(12, 7));
    vloadC_int32(C38, &Crref(12, 8));
    vloadC_int32(C39, &Crref(12, 9));
    vloadC_int32(C310, &Crref(12, 10));
    vloadC_int32(C311, &Crref(12, 11));
    vloadC_int32(C312, &Crref(12, 12));
    vloadC_int32(C313, &Crref(12, 13));
    vloadC_int32(C314, &Crref(12, 14));
    vloadC_int32(C315, &Crref(12, 15));
    vloadC_int32(C316, &Crref(12, 16));
    vloadC_int32(C317, &Crref(12, 17));
    vloadC_int32(C318, &Crref(12, 18));
    vloadC_int32(C319, &Crref(12, 19));
    vloadC_int32(C40, &Crref(16, 0));
    vloadC_int32(C41, &Crref(16, 1));
    vloadC_int32(C42, &Crref(16, 2));
    vloadC_int32(C43, &Crref(16, 3));
    vloadC_int32(C44, &Crref(16, 4));
    vloadC_int32(C45, &Crref(16, 5));
    vloadC_int32(C46, &Crref(16, 6));
    vloadC_int32(C47, &Crref(16, 7));
    vloadC_int32(C48, &Crref(16, 8));
    vloadC_int32(C49, &Crref(16, 9));
    vloadC_int32(C410, &Crref(16, 10));
    vloadC_int32(C411, &Crref(16, 11));
    vloadC_int32(C412, &Crref(16, 12));
    vloadC_int32(C413, &Crref(16, 13));
    vloadC_int32(C414, &Crref(16, 14));
    vloadC_int32(C415, &Crref(16, 15));
    vloadC_int32(C416, &Crref(16, 16));
    vloadC_int32(C417, &Crref(16, 17));
    vloadC_int32(C418, &Crref(16, 18));
    vloadC_int32(C419, &Crref(16, 19));
  }

  for (pr=0; pr<kc; pr++) { // Loop L6
    vload_int32(A0, &Ar[bA + 0]);
    vload_int32(A1, &Ar[bA + 4]);
    vload_int32(A2, &Ar[bA + 8]);
    vload_int32(A3, &Ar[bA + 12]);
    vload_int32(A4, &Ar[bA + 16]);
    vload_int32(B0, &Br[bB + 0]);
    vload_int32(B1, &Br[bB + 4]);
    vload_int32(B2, &Br[bB + 8]);
    vload_int32(B3, &Br[bB + 12]);
    vload_int32(B4, &Br[bB + 16]);
    vupdate_lane_int32(C00, A0, B0, 0); 
    vupdate_lane_int32(C01, A0, B0, 1); 
    vupdate_lane_int32(C02, A0, B0, 2); 
    vupdate_lane_int32(C03, A0, B0, 3); 
    vupdate_lane_int32(C04, A0, B1, 0); 
    vupdate_lane_int32(C05, A0, B1, 1); 
    vupdate_lane_int32(C06, A0, B1, 2); 
    vupdate_lane_int32(C07, A0, B1, 3); 
    vupdate_lane_int32(C08, A0, B2, 0); 
    vupdate_lane_int32(C09, A0, B2, 1); 
    vupdate_lane_int32(C010, A0, B2, 2); 
    vupdate_lane_int32(C011, A0, B2, 3); 
    vupdate_lane_int32(C012, A0, B3, 0); 
    vupdate_lane_int32(C013, A0, B3, 1); 
    vupdate_lane_int32(C014, A0, B3, 2); 
    vupdate_lane_int32(C015, A0, B3, 3); 
    vupdate_lane_int32(C016, A0, B4, 0); 
    vupdate_lane_int32(C017, A0, B4, 1); 
    vupdate_lane_int32(C018, A0, B4, 2); 
    vupdate_lane_int32(C019, A0, B4, 3); 
    vupdate_lane_int32(C10, A1, B0, 0); 
    vupdate_lane_int32(C11, A1, B0, 1); 
    vupdate_lane_int32(C12, A1, B0, 2); 
    vupdate_lane_int32(C13, A1, B0, 3); 
    vupdate_lane_int32(C14, A1, B1, 0); 
    vupdate_lane_int32(C15, A1, B1, 1); 
    vupdate_lane_int32(C16, A1, B1, 2); 
    vupdate_lane_int32(C17, A1, B1, 3); 
    vupdate_lane_int32(C18, A1, B2, 0); 
    vupdate_lane_int32(C19, A1, B2, 1); 
    vupdate_lane_int32(C110, A1, B2, 2); 
    vupdate_lane_int32(C111, A1, B2, 3); 
    vupdate_lane_int32(C112, A1, B3, 0); 
    vupdate_lane_int32(C113, A1, B3, 1); 
    vupdate_lane_int32(C114, A1, B3, 2); 
    vupdate_lane_int32(C115, A1, B3, 3); 
    vupdate_lane_int32(C116, A1, B4, 0); 
    vupdate_lane_int32(C117, A1, B4, 1); 
    vupdate_lane_int32(C118, A1, B4, 2); 
    vupdate_lane_int32(C119, A1, B4, 3); 
    vupdate_lane_int32(C20, A2, B0, 0); 
    vupdate_lane_int32(C21, A2, B0, 1); 
    vupdate_lane_int32(C22, A2, B0, 2); 
    vupdate_lane_int32(C23, A2, B0, 3); 
    vupdate_lane_int32(C24, A2, B1, 0); 
    vupdate_lane_int32(C25, A2, B1, 1); 
    vupdate_lane_int32(C26, A2, B1, 2); 
    vupdate_lane_int32(C27, A2, B1, 3); 
    vupdate_lane_int32(C28, A2, B2, 0); 
    vupdate_lane_int32(C29, A2, B2, 1); 
    vupdate_lane_int32(C210, A2, B2, 2); 
    vupdate_lane_int32(C211, A2, B2, 3); 
    vupdate_lane_int32(C212, A2, B3, 0); 
    vupdate_lane_int32(C213, A2, B3, 1); 
    vupdate_lane_int32(C214, A2, B3, 2); 
    vupdate_lane_int32(C215, A2, B3, 3); 
    vupdate_lane_int32(C216, A2, B4, 0); 
    vupdate_lane_int32(C217, A2, B4, 1); 
    vupdate_lane_int32(C218, A2, B4, 2); 
    vupdate_lane_int32(C219, A2, B4, 3); 
    vupdate_lane_int32(C30, A3, B0, 0); 
    vupdate_lane_int32(C31, A3, B0, 1); 
    vupdate_lane_int32(C32, A3, B0, 2); 
    vupdate_lane_int32(C33, A3, B0, 3); 
    vupdate_lane_int32(C34, A3, B1, 0); 
    vupdate_lane_int32(C35, A3, B1, 1); 
    vupdate_lane_int32(C36, A3, B1, 2); 
    vupdate_lane_int32(C37, A3, B1, 3); 
    vupdate_lane_int32(C38, A3, B2, 0); 
    vupdate_lane_int32(C39, A3, B2, 1); 
    vupdate_lane_int32(C310, A3, B2, 2); 
    vupdate_lane_int32(C311, A3, B2, 3); 
    vupdate_lane_int32(C312, A3, B3, 0); 
    vupdate_lane_int32(C313, A3, B3, 1); 
    vupdate_lane_int32(C314, A3, B3, 2); 
    vupdate_lane_int32(C315, A3, B3, 3); 
    vupdate_lane_int32(C316, A3, B4, 0); 
    vupdate_lane_int32(C317, A3, B4, 1); 
    vupdate_lane_int32(C318, A3, B4, 2); 
    vupdate_lane_int32(C319, A3, B4, 3); 
    vupdate_lane_int32(C40, A4, B0, 0); 
    vupdate_lane_int32(C41, A4, B0, 1); 
    vupdate_lane_int32(C42, A4, B0, 2); 
    vupdate_lane_int32(C43, A4, B0, 3); 
    vupdate_lane_int32(C44, A4, B1, 0); 
    vupdate_lane_int32(C45, A4, B1, 1); 
    vupdate_lane_int32(C46, A4, B1, 2); 
    vupdate_lane_int32(C47, A4, B1, 3); 
    vupdate_lane_int32(C48, A4, B2, 0); 
    vupdate_lane_int32(C49, A4, B2, 1); 
    vupdate_lane_int32(C410, A4, B2, 2); 
    vupdate_lane_int32(C411, A4, B2, 3); 
    vupdate_lane_int32(C412, A4, B3, 0); 
    vupdate_lane_int32(C413, A4, B3, 1); 
    vupdate_lane_int32(C414, A4, B3, 2); 
    vupdate_lane_int32(C415, A4, B3, 3); 
    vupdate_lane_int32(C416, A4, B4, 0); 
    vupdate_lane_int32(C417, A4, B4, 1); 
    vupdate_lane_int32(C418, A4, B4, 2); 
    vupdate_lane_int32(C419, A4, B4, 3); 
    bA+=20;
    bB+=20;
  }

  vstoreC_int32(&Crref(0,0), C00); 
  vstoreC_int32(&Crref(0,1), C01); 
  vstoreC_int32(&Crref(0,2), C02); 
  vstoreC_int32(&Crref(0,3), C03); 
  vstoreC_int32(&Crref(0,4), C04); 
  vstoreC_int32(&Crref(0,5), C05); 
  vstoreC_int32(&Crref(0,6), C06); 
  vstoreC_int32(&Crref(0,7), C07); 
  vstoreC_int32(&Crref(0,8), C08); 
  vstoreC_int32(&Crref(0,9), C09); 
  vstoreC_int32(&Crref(0,10), C010); 
  vstoreC_int32(&Crref(0,11), C011); 
  vstoreC_int32(&Crref(0,12), C012); 
  vstoreC_int32(&Crref(0,13), C013); 
  vstoreC_int32(&Crref(0,14), C014); 
  vstoreC_int32(&Crref(0,15), C015); 
  vstoreC_int32(&Crref(0,16), C016); 
  vstoreC_int32(&Crref(0,17), C017); 
  vstoreC_int32(&Crref(0,18), C018); 
  vstoreC_int32(&Crref(0,19), C019); 
  vstoreC_int32(&Crref(4,0), C10); 
  vstoreC_int32(&Crref(4,1), C11); 
  vstoreC_int32(&Crref(4,2), C12); 
  vstoreC_int32(&Crref(4,3), C13); 
  vstoreC_int32(&Crref(4,4), C14); 
  vstoreC_int32(&Crref(4,5), C15); 
  vstoreC_int32(&Crref(4,6), C16); 
  vstoreC_int32(&Crref(4,7), C17); 
  vstoreC_int32(&Crref(4,8), C18); 
  vstoreC_int32(&Crref(4,9), C19); 
  vstoreC_int32(&Crref(4,10), C110); 
  vstoreC_int32(&Crref(4,11), C111); 
  vstoreC_int32(&Crref(4,12), C112); 
  vstoreC_int32(&Crref(4,13), C113); 
  vstoreC_int32(&Crref(4,14), C114); 
  vstoreC_int32(&Crref(4,15), C115); 
  vstoreC_int32(&Crref(4,16), C116); 
  vstoreC_int32(&Crref(4,17), C117); 
  vstoreC_int32(&Crref(4,18), C118); 
  vstoreC_int32(&Crref(4,19), C119); 
  vstoreC_int32(&Crref(8,0), C20); 
  vstoreC_int32(&Crref(8,1), C21); 
  vstoreC_int32(&Crref(8,2), C22); 
  vstoreC_int32(&Crref(8,3), C23); 
  vstoreC_int32(&Crref(8,4), C24); 
  vstoreC_int32(&Crref(8,5), C25); 
  vstoreC_int32(&Crref(8,6), C26); 
  vstoreC_int32(&Crref(8,7), C27); 
  vstoreC_int32(&Crref(8,8), C28); 
  vstoreC_int32(&Crref(8,9), C29); 
  vstoreC_int32(&Crref(8,10), C210); 
  vstoreC_int32(&Crref(8,11), C211); 
  vstoreC_int32(&Crref(8,12), C212); 
  vstoreC_int32(&Crref(8,13), C213); 
  vstoreC_int32(&Crref(8,14), C214); 
  vstoreC_int32(&Crref(8,15), C215); 
  vstoreC_int32(&Crref(8,16), C216); 
  vstoreC_int32(&Crref(8,17), C217); 
  vstoreC_int32(&Crref(8,18), C218); 
  vstoreC_int32(&Crref(8,19), C219); 
  vstoreC_int32(&Crref(12,0), C30); 
  vstoreC_int32(&Crref(12,1), C31); 
  vstoreC_int32(&Crref(12,2), C32); 
  vstoreC_int32(&Crref(12,3), C33); 
  vstoreC_int32(&Crref(12,4), C34); 
  vstoreC_int32(&Crref(12,5), C35); 
  vstoreC_int32(&Crref(12,6), C36); 
  vstoreC_int32(&Crref(12,7), C37); 
  vstoreC_int32(&Crref(12,8), C38); 
  vstoreC_int32(&Crref(12,9), C39); 
  vstoreC_int32(&Crref(12,10), C310); 
  vstoreC_int32(&Crref(12,11), C311); 
  vstoreC_int32(&Crref(12,12), C312); 
  vstoreC_int32(&Crref(12,13), C313); 
  vstoreC_int32(&Crref(12,14), C314); 
  vstoreC_int32(&Crref(12,15), C315); 
  vstoreC_int32(&Crref(12,16), C316); 
  vstoreC_int32(&Crref(12,17), C317); 
  vstoreC_int32(&Crref(12,18), C318); 
  vstoreC_int32(&Crref(12,19), C319); 
  vstoreC_int32(&Crref(16,0), C40); 
  vstoreC_int32(&Crref(16,1), C41); 
  vstoreC_int32(&Crref(16,2), C42); 
  vstoreC_int32(&Crref(16,3), C43); 
  vstoreC_int32(&Crref(16,4), C44); 
  vstoreC_int32(&Crref(16,5), C45); 
  vstoreC_int32(&Crref(16,6), C46); 
  vstoreC_int32(&Crref(16,7), C47); 
  vstoreC_int32(&Crref(16,8), C48); 
  vstoreC_int32(&Crref(16,9), C49); 
  vstoreC_int32(&Crref(16,10), C410); 
  vstoreC_int32(&Crref(16,11), C411); 
  vstoreC_int32(&Crref(16,12), C412); 
  vstoreC_int32(&Crref(16,13), C413); 
  vstoreC_int32(&Crref(16,14), C414); 
  vstoreC_int32(&Crref(16,15), C415); 
  vstoreC_int32(&Crref(16,16), C416); 
  vstoreC_int32(&Crref(16,17), C417); 
  vstoreC_int32(&Crref(16,18), C418); 
  vstoreC_int32(&Crref(16,19), C419); 
  if (mr != MR || nr != NR)
    for (int j = 0; j < nr; j++)
      for (int i = 0; i < mr; i++)
        Cor[j*Clda + i] = (betaI) * Cor[j*Clda + i] + Ctmp[j * MR + i];
}

uk_intrinsic_int32 *new_uk_intrinsic_selector_int32() { 
  uk_intrinsic_int32 *uk_vec = (uk_intrinsic_int32 *)malloc(sizeof(uk_intrinsic_int32) * 21 * 21);
  uk_vec[4*21 + 4] = ukernel_intrinsic_4x4_int32;
  uk_vec[8*21 + 4] = ukernel_intrinsic_4x8_int32;
  uk_vec[12*21 + 4] = ukernel_intrinsic_4x12_int32;
  uk_vec[16*21 + 4] = ukernel_intrinsic_4x16_int32;
  uk_vec[20*21 + 4] = ukernel_intrinsic_4x20_int32;
  uk_vec[4*21 + 8] = ukernel_intrinsic_8x4_int32;
  uk_vec[8*21 + 8] = ukernel_intrinsic_8x8_int32;
  uk_vec[12*21 + 8] = ukernel_intrinsic_8x12_int32;
  uk_vec[16*21 + 8] = ukernel_intrinsic_8x16_int32;
  uk_vec[20*21 + 8] = ukernel_intrinsic_8x20_int32;
  uk_vec[4*21 + 12] = ukernel_intrinsic_12x4_int32;
  uk_vec[8*21 + 12] = ukernel_intrinsic_12x8_int32;
  uk_vec[12*21 + 12] = ukernel_intrinsic_12x12_int32;
  uk_vec[16*21 + 12] = ukernel_intrinsic_12x16_int32;
  uk_vec[20*21 + 12] = ukernel_intrinsic_12x20_int32;
  uk_vec[4*21 + 16] = ukernel_intrinsic_16x4_int32;
  uk_vec[8*21 + 16] = ukernel_intrinsic_16x8_int32;
  uk_vec[12*21 + 16] = ukernel_intrinsic_16x12_int32;
  uk_vec[16*21 + 16] = ukernel_intrinsic_16x16_int32;
  uk_vec[20*21 + 16] = ukernel_intrinsic_16x20_int32;
  uk_vec[4*21 + 20] = ukernel_intrinsic_20x4_int32;
  uk_vec[8*21 + 20] = ukernel_intrinsic_20x8_int32;
  uk_vec[12*21 + 20] = ukernel_intrinsic_20x12_int32;
  uk_vec[16*21 + 20] = ukernel_intrinsic_20x16_int32;
  uk_vec[20*21 + 20] = ukernel_intrinsic_20x20_int32;
  return uk_vec;
}

void uk_intrinsic_selector_int32(int mr, int nr, uk_intrinsic_int32 *uk_vec, uk_intrinsic_int32 *ukr) {
  (*ukr) = uk_vec[nr*21 + mr];
}

uk_config_int32_t *new_uk_intrinsic_config_int32() {
  uk_config_int32_t *uk_config = (uk_config_int32_t *)malloc(sizeof(uk_config_int32_t));
  uk_config->uk_num = 0;
  uk_config->mr_pool[uk_config->uk_num] = 4;
  uk_config->nr_pool[uk_config->uk_num] = 4;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 4;
  uk_config->nr_pool[uk_config->uk_num] = 8;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 4;
  uk_config->nr_pool[uk_config->uk_num] = 12;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 4;
  uk_config->nr_pool[uk_config->uk_num] = 16;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 4;
  uk_config->nr_pool[uk_config->uk_num] = 20;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 8;
  uk_config->nr_pool[uk_config->uk_num] = 4;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 8;
  uk_config->nr_pool[uk_config->uk_num] = 8;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 8;
  uk_config->nr_pool[uk_config->uk_num] = 12;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 8;
  uk_config->nr_pool[uk_config->uk_num] = 16;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 8;
  uk_config->nr_pool[uk_config->uk_num] = 20;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 12;
  uk_config->nr_pool[uk_config->uk_num] = 4;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 12;
  uk_config->nr_pool[uk_config->uk_num] = 8;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 12;
  uk_config->nr_pool[uk_config->uk_num] = 12;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 12;
  uk_config->nr_pool[uk_config->uk_num] = 16;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 12;
  uk_config->nr_pool[uk_config->uk_num] = 20;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 16;
  uk_config->nr_pool[uk_config->uk_num] = 4;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 16;
  uk_config->nr_pool[uk_config->uk_num] = 8;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 16;
  uk_config->nr_pool[uk_config->uk_num] = 12;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 16;
  uk_config->nr_pool[uk_config->uk_num] = 16;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 16;
  uk_config->nr_pool[uk_config->uk_num] = 20;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 20;
  uk_config->nr_pool[uk_config->uk_num] = 4;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 20;
  uk_config->nr_pool[uk_config->uk_num] = 8;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 20;
  uk_config->nr_pool[uk_config->uk_num] = 12;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 20;
  uk_config->nr_pool[uk_config->uk_num] = 16;
  uk_config->uk_num++;
  uk_config->mr_pool[uk_config->uk_num] = 20;
  uk_config->nr_pool[uk_config->uk_num] = 20;
  uk_config->uk_num++;
  return uk_config;
}

